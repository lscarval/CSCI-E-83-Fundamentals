{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "88f06c4d",
      "metadata": {
        "id": "88f06c4d"
      },
      "source": [
        "# CSCI E-83 Final Project\n",
        "\n",
        "## Pandemic Recovery Economic Indicators\n",
        "\n",
        "### **Student:** Luciano Carvalho\n",
        "\n",
        "This project leverages the Opportunity Insights Economic Tracker dataset to explore relationships between key economic indicators. By cross-referencing multiple datasets, the project aims to uncover actionable insights into economic behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae55979a",
      "metadata": {
        "id": "ae55979a"
      },
      "source": [
        "## Executive Summary\n",
        "\n",
        "The Opportunity Insights Economic Tracker dataset provides a comprehensive view of economic activity, including open-source data on pandemic lockdowns and post-pandemic recovery, alongside employment trends, consumer spending, and education metrics. This project focuses on cross-referencing these datasets to explore relationships and understand the influences of key indicators, such as the economic impacts of government interventions, reopening policies, and long-term recovery trends. The goal is to uncover actionable insights that can inform economic policies and decision-making for future crisis preparedness and sustained growth."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78c8bdb0",
      "metadata": {
        "id": "78c8bdb0"
      },
      "source": [
        "## Problem Statement\n",
        "\n",
        "Economic indicators like employment, spending, and education are interrelated. Understanding these relationships is crucial for designing effective policies. This project aims to:\n",
        "- Explore temporal and geographic patterns in economic data.\n",
        "- Cross-reference datasets to identify correlations and causations.\n",
        "- Provide actionable insights into economic trends and policy impacts."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7ec2f43",
      "metadata": {
        "id": "c7ec2f43"
      },
      "source": [
        "## Data Description\n",
        "\n",
        "The project utilizes datasets from the Opportunity Insights Economic Tracker. Key datasets include:\n",
        "- **Affinity Data**: Daily consumer spending metrics.\n",
        "- **Employment Data**: Weekly employment rates segmented by income and industry.\n",
        "- **UI Claims Data**: Weekly unemployment insurance claims.\n",
        "- **Womply Data**: Weekly business activity metrics.\n",
        "- **Zearn Data**: Weekly education engagement metrics.\n",
        "\n",
        "The datasets provide rich numeric variables and extensive temporal coverage, enabling detailed exploratory and inferential analysis.\n",
        "\n",
        "The full dataset can be found at: https://github.com/OpportunityInsights/EconomicTracker"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87094de3",
      "metadata": {
        "id": "87094de3"
      },
      "source": [
        "# Exploratory Data Analysis (EDA), and Data Preparation and Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OLpLhlK4gt0f",
      "metadata": {
        "id": "OLpLhlK4gt0f"
      },
      "outputs": [],
      "source": [
        "# Load required libraries\n",
        "from matplotlib.dates import DateFormatter\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "from statsmodels.stats.stattools import durbin_watson\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "import seaborn as sns\n",
        "import zipfile\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AT6DzDJTV8mc",
      "metadata": {
        "id": "AT6DzDJTV8mc"
      },
      "source": [
        "The following code snippets illustrate the preliminary exploratory analysis performed on the dataset to understand its structure, identify key patterns, and prepare it for modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ojq4aigkh_u9",
      "metadata": {
        "id": "ojq4aigkh_u9"
      },
      "outputs": [],
      "source": [
        "# Path to the zip file and the destination folder\n",
        "zip_file_path = '/content/EconomicTracker-main.zip'  # Update with the correct path if needed\n",
        "extraction_path = '/content/dataset/'  # Folder to extract the files to\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extraction_path)\n",
        "\n",
        "print(f\"Files extracted to: {extraction_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Metadata\n",
        "\n",
        "- **Affinity Data**: Tracks daily spending segmented by income groups.\n",
        "- **Employment Data**: Weekly employment rates segmented by income quartiles and industries.\n",
        "- **UI Claims Data**: Weekly unemployment claims showing economic shock and recovery patterns.\n",
        "- **Womply Data**: Business activity trends by revenue and merchant counts.\n",
        "- **Zearn Data**: Education engagement and achievement by income levels."
      ],
      "metadata": {
        "id": "DNt2zC7nEs5j"
      },
      "id": "DNt2zC7nEs5j"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UaXBJ_O_qYLa",
      "metadata": {
        "id": "UaXBJ_O_qYLa"
      },
      "outputs": [],
      "source": [
        "# Path to the data directory\n",
        "data_dir = '/content/dataset/EconomicTracker-main/data'\n",
        "\n",
        "# Dictionary to store category-wise file counts\n",
        "category_file_counts = {}\n",
        "\n",
        "# Iterate through the files in the directory\n",
        "for file_name in os.listdir(data_dir):\n",
        "    # Check if the file is a CSV\n",
        "    if file_name.endswith('.csv'):\n",
        "        # Extract the category from the file name (assumes categories are separated by \" - \")\n",
        "        category = file_name.split(' - ')[0]\n",
        "        category_file_counts[category] = category_file_counts.get(category, 0) + 1\n",
        "\n",
        "# Print the counts for each category\n",
        "print(\"CSV File Counts by Category:\")\n",
        "for category, count in category_file_counts.items():\n",
        "    print(f\"{category}: {count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Qb9aTskpkpN9",
      "metadata": {
        "id": "Qb9aTskpkpN9"
      },
      "source": [
        "### Data Categories\n",
        "\n",
        "The dataset comprises several categories of data, each reflecting distinct aspects of economic activity and indicators:\n",
        "\n",
        "1. **COVID Data:** Includes metrics related to pandemic effects, such as case counts, deaths, and policies.\n",
        "2. **GeoIDs:** Provides location-specific identifiers for mapping and cross-referencing across datasets.\n",
        "3. **Employment:** Tracks employment rates at various geographic and temporal levels.\n",
        "4. **UI Claims:** Data on unemployment insurance claims, reflecting labor market stress.\n",
        "5. **Google Mobility:** Captures mobility patterns using Google location data.\n",
        "6. **Womply:** Reflects business transaction data, offering insights into small business health.\n",
        "7. **Affinity Daily Total Spending:** Tracks daily consumer spending patterns nationally.\n",
        "8. **Job Postings:** Weekly data on job listings, reflecting labor demand.\n",
        "9. **Zearn:** Education-related data, offering insights into student progress.\n",
        "10. **Policy Milestones:** Tracks policy changes over time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EryfqYiaONws",
      "metadata": {
        "id": "EryfqYiaONws"
      },
      "source": [
        "**Focus Areas:**\n",
        "- Cross-referencing **consumer spending** (Affinity) with **employment**, **UI Claims**, and **COVID** data to explore interdependencies.\n",
        "- Investigating **mobility trends** (Google Mobility) and their relationship with consumer spending and business activity (Womply).\n",
        "- Analyzing **job postings** trends to correlate with unemployment data and spending activity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hfNDDxeMowpH",
      "metadata": {
        "id": "hfNDDxeMowpH"
      },
      "outputs": [],
      "source": [
        "# Load and preview Affinity Data\n",
        "affinity_data_path = f\"{data_dir}/Affinity - National - Daily.csv\"\n",
        "affinity_data = pd.read_csv(affinity_data_path)\n",
        "print(affinity_data.info())\n",
        "\n",
        "# Check the column names in the dataset\n",
        "print(list(affinity_data.columns))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GPmdH4o4p_Ki",
      "metadata": {
        "id": "GPmdH4o4p_Ki"
      },
      "source": [
        "The dataset contains a wide variety of fields. The **`Affinity - National - Daily.csv`** file alone has 142 columns, representing consumer spending broken down by income quartiles, categories, and time periods.\n",
        "\n",
        "**Key fields include:**\n",
        "\n",
        "- `spend_all`: Total spending across all categories.\n",
        "- `spend_all_q1`, `spend_all_q2`, etc.: Spending by income quartiles.\n",
        "- `spend_retail_no_grocery`, `spend_retail_w_grocery`: Spending in retail with/without groceries.\n",
        "\n",
        "These fields allow us to analyze national trends and break them down into more granular segments.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a_PdCw0GrZm",
      "metadata": {
        "id": "5a_PdCw0GrZm"
      },
      "outputs": [],
      "source": [
        "# Create a date column\n",
        "affinity_data['date'] = pd.to_datetime(affinity_data[['year', 'month', 'day']])\n",
        "\n",
        "# Collect the first and last observation dates\n",
        "first_date = affinity_data['date'].min()\n",
        "last_date = affinity_data['date'].max()\n",
        "\n",
        "# Display the results\n",
        "print(f\"First observed date: {first_date}\")\n",
        "print(f\"Last observed date: {last_date}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09oo_F7cPX6D",
      "metadata": {
        "id": "09oo_F7cPX6D"
      },
      "source": [
        "#### Passible Cross-Referencing Paths\n",
        "\n",
        "- Match the `year`, `month`, and `day` fields across datasets for temporal alignment.\n",
        "- Use `GeoIDs` for location-based cross-referencing.\n",
        "- Correlate consumer spending with employment data to identify economic patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2QDmbtF3KNK8",
      "metadata": {
        "id": "2QDmbtF3KNK8"
      },
      "outputs": [],
      "source": [
        "# Identify the first value\n",
        "initial_value = affinity_data['spend_all'].iloc[0]\n",
        "\n",
        "# Find the first index where the value changes\n",
        "first_change_index = affinity_data['spend_all'][affinity_data['spend_all'] != initial_value].index[0]\n",
        "\n",
        "# Trim the dataset from the first change onward\n",
        "affinity_data = affinity_data.loc[first_change_index:]\n",
        "\n",
        "# Verify the trimming\n",
        "print(f\"Data starts at index: {affinity_data.index[0]}\")\n",
        "print(affinity_data.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y6SwOBNNKyWv",
      "metadata": {
        "id": "y6SwOBNNKyWv"
      },
      "source": [
        "In the initial exploration of the dataset, it was observed that the time series contained non-zero values prior to January 14, 2020 ($spend\\_all=-0.0146$). However, these values were constant and insignificant, indicating either interpolated or placeholder data rather than meaningful observations. Such flat sections at the start of a time series can artificially distort trends and seasonal decomposition analyses by introducing an extended period of non-informative data.\n",
        "\n",
        "To address this, we have trimmed the dataset to begin from January 14, 2020, which marks the first date with meaningful variability in the observed data. By removing the insignificant values, we ensure that the analysis reflects actual economic trends and behaviors, providing more reliable insights. This step is particularly crucial for accurate trend extraction, correlation studies, and any predictive modeling applied to the data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W89HlXhaswHa",
      "metadata": {
        "id": "W89HlXhaswHa"
      },
      "outputs": [],
      "source": [
        "# Set the 'date' column as the index\n",
        "affinity_data.set_index('date', inplace=True)\n",
        "\n",
        "# Ensure there are no missing dates in the index by reindexing with a complete date range\n",
        "full_date_range = pd.date_range(start=affinity_data.index.min(), end=affinity_data.index.max(), freq='D')\n",
        "affinity_data = affinity_data.reindex(full_date_range)\n",
        "\n",
        "# Fill missing values in 'spend_all' with linear interpolation\n",
        "affinity_data['spend_all'] = pd.to_numeric(affinity_data['spend_all'], errors='coerce')\n",
        "affinity_data['spend_all'] = affinity_data['spend_all'].interpolate(method='linear', limit_direction='both')\n",
        "\n",
        "# Trim rows with null or zero values at the start of the dataset\n",
        "first_valid_date = affinity_data['spend_all'].first_valid_index()\n",
        "affinity_data = affinity_data.loc[first_valid_date:]\n",
        "\n",
        "# Verify variance in 'spend_all'\n",
        "print(affinity_data['spend_all'].describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DvQvnNFgVFV_",
      "metadata": {
        "id": "DvQvnNFgVFV_"
      },
      "source": [
        "## High-level Data Description\n",
        "\n",
        "The **`Affinity - National - Daily.csv`** file contains 1,617 daily records of consumer spending across various income quartiles and categories. Here's a summary of the `spend_all` column, representing total spending across all categories:\n",
        "\n",
        "- **Mean Spending:** $0.097$, indicating the average scaled spending per day.\n",
        "- **Standard Deviation:** $0.111$, suggesting moderate variation in daily spending.\n",
        "- **Min/Max Values:** Range from $-0.335$ to $0.255$ (scaled values). Negative values may reflect data adjustments or anomalies.\n",
        "- **Interquartile Range (IQR):** Spending is concentrated between $0.054$ (25th percentile) and $0.177$ (75th percentile).\n",
        "\n",
        "The data has been normalized using Min-Max scaling to fit between $0$ and $1$, improving the decomposition's clarity and interpretability.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k73z6eyHVEZk",
      "metadata": {
        "id": "k73z6eyHVEZk"
      },
      "outputs": [],
      "source": [
        "# Calculate a rolling mean to represent the trend\n",
        "affinity_data['trend'] = affinity_data['spend_all'].rolling(window=30, center=True).mean()\n",
        "\n",
        "# Calculate residuals as the difference between spend_all and the trend\n",
        "affinity_data['residuals'] = affinity_data['spend_all'] - affinity_data['trend']\n",
        "\n",
        "# Drop rows with NaN values resulting from the rolling mean\n",
        "affinity_data_cleaned = affinity_data.dropna(subset=['trend', 'residuals'])\n",
        "\n",
        "# Create a 2x2 grid of plots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# Time Series Plot\n",
        "axes[0, 0].plot(affinity_data_cleaned.index, affinity_data_cleaned['spend_all'], label='Spending')\n",
        "axes[0, 0].plot(affinity_data_cleaned.index, affinity_data_cleaned['trend'], label='Trend', color='red', linestyle='--')\n",
        "axes[0, 0].set_title('Time Series Plot with Trend')\n",
        "axes[0, 0].set_xlabel('Date')\n",
        "axes[0, 0].set_ylabel('Spending')\n",
        "axes[0, 0].grid(axis='y', linestyle='--', alpha=0.7)\n",
        "axes[0, 0].legend()\n",
        "\n",
        "# Histogram of Residuals\n",
        "sns.histplot(affinity_data_cleaned['residuals'], bins=30, kde=True, color='blue', ax=axes[0, 1])\n",
        "axes[0, 1].set_title('Histogram of Residuals')\n",
        "axes[0, 1].set_xlabel('Residuals')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Q-Q Plot\n",
        "stats.probplot(affinity_data_cleaned['residuals'], dist=\"norm\", plot=axes[1, 0])\n",
        "axes[1, 0].set_title('Q-Q Plot of Residuals')\n",
        "axes[1, 0].grid(alpha=0.3)\n",
        "\n",
        "# Residuals Plot\n",
        "axes[1, 1].scatter(affinity_data_cleaned.index, affinity_data_cleaned['residuals'], alpha=0.7, label='Residuals')\n",
        "axes[1, 1].axhline(0, color='red', linestyle='--', label='Mean Line')\n",
        "axes[1, 1].set_title('Residuals Plot')\n",
        "axes[1, 1].set_xlabel('Date')\n",
        "axes[1, 1].set_ylabel('Residuals')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(alpha=0.3)\n",
        "\n",
        "# Adjust layout and display the plots\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary Statistics\n",
        "print(\"Summary Statistics of Residuals:\")\n",
        "print(affinity_data_cleaned['residuals'].describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lYthwXecMbjO",
      "metadata": {
        "id": "lYthwXecMbjO"
      },
      "source": [
        "### Analysis of Key Plots for Trend and Residual Behavior\n",
        "\n",
        "- **Time Series Plot with Trend**  \n",
        "  -  The top-left plot displays the time series of total consumer spending (`spend_all`) over time, with the red dashed line indicating the trend component. This trend was calculated using a rolling average with a window size of 30 days to smooth out short-term fluctuations. The plot highlights the significant drop in spending in early 2020, likely caused by the onset of the COVID-19 pandemic, followed by a steady recovery through 2021 and beyond. The trend stabilizes after mid-2022, reflecting the normalization of consumer behavior.\n",
        "\n",
        "- **Histogram of Residuals**  \n",
        "  - The top-right plot illustrates the distribution of residuals, calculated as the difference between actual spending values and the trend. The histogram is overlaid with a kernel density estimate (KDE) to visualize the data's distribution. The residuals appear to be centered around zero, indicating that the trend adequately captures the overall pattern in the data. The near-symmetrical shape suggests that the deviations from the trend are relatively balanced, though small peaks hint at potential outliers or unusual periods.\n",
        "\n",
        "- **Q-Q Plot of Residuals**  \n",
        "  - The bottom-left plot is a Q-Q comparing the residuals' distribution to a theoretical normal distribution. Most points align closely with the red diagonal line, indicating that the residuals are approximately normally distributed. However, deviations at the tails suggest that there may be some outliers or non-normality in the data, potentially during periods of abrupt economic changes.\n",
        "\n",
        "- **Residuals Plot**  \n",
        "  - The bottom-right plot shows the residuals over time. The red dashed line represents the mean of the residuals, which is close to zero, confirming that the trend component has been effectively removed. The scatter of residuals does not exhibit any obvious patterns, which is desirable in time series analysis. However, slight clustering of points in certain time periods suggests there could be unexplained variations during those intervals, possibly due to external factors such as economic policies or public health measures.\n",
        "\n",
        "These plots collectively validate the effectiveness of the trend extraction process while providing insights into the data's variability and deviations. The residuals' normality and lack of clear temporal patterns indicate a sound basis for further statistical modeling or hypothesis testing."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wqUPVTGHVPLn",
      "metadata": {
        "id": "wqUPVTGHVPLn"
      },
      "source": [
        "### Explanation for Not Considering Seasonal Decomposition\n",
        "\n",
        "In the analysis, I decided to move away from incorporating seasonal decomposition because a thorough examination of the data revealed no significant seasonal patterns. Seasonal decomposition is typically used when the data exhibits consistent recurring patterns over specific time periods (e.g., monthly or yearly cycles). However, both visual inspection of the time series and the results of the seasonal decomposition process indicated that the seasonal component was negligible in this dataset.\n",
        "\n",
        "When applying seasonal decomposition, the seasonal component was forced to fit the data, resulting in coefficients that lacked clear relevance or meaningful interpretation. This is likely due to the nature of the dataset, which reflects economic factors and consumer spending trends influenced by external disruptions such as the COVID-19 pandemic. These disruptions are irregular and non-recurring, making seasonal patterns difficult, if not impossible, to detect.\n",
        "\n",
        "By focusing on the trend and residual components, I can better address the significant changes in spending driven by major events and long-term trends, rather than attempting to identify cyclical behaviors that do not exist in the data. This approach aligns with the goal of understanding the key drivers of consumer behavior and exploring relationships between spending, employment, and other economic indicators. Removing seasonal decomposition simplifies the analysis and ensures that the findings are meaningful and relevant to the context of this project."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZkOWGBRtZGkd",
      "metadata": {
        "id": "ZkOWGBRtZGkd"
      },
      "source": [
        "## Additional Data Exploration and Preprocessing\n",
        "\n",
        "This section delves deeper into the dataset through additional exploratory visualizations, data cleaning, and preprocessing steps to ensure it is well-prepared for subsequent modeling and analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8uWrz9ANfGhs",
      "metadata": {
        "id": "8uWrz9ANfGhs"
      },
      "source": [
        "### Bloxplots of `spend_all` per year\n",
        "\n",
        "This subsection presents boxplots of spend_all for each year, providing a visual summary of the distribution and variability in total spending over time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bftmekqgrVlF",
      "metadata": {
        "id": "bftmekqgrVlF"
      },
      "outputs": [],
      "source": [
        "# Filter data for years 2020 to 2023\n",
        "filtered_affinity_data = affinity_data[(affinity_data['year'] >= 2020) & (affinity_data['year'] <= 2023)]\n",
        "\n",
        "# Plot boxplots of 'spend_all' for each year\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=filtered_affinity_data, x='year', y='spend_all', hue='year', palette='coolwarm', dodge=False, legend=False)\n",
        "plt.title('Boxplot of Total Consumer Spending (spend_all) Per Year (2020-2023)')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Total Consumer Spending')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_RUG8yYGsZSy",
      "metadata": {
        "id": "_RUG8yYGsZSy"
      },
      "source": [
        "The boxplot highlights a significant dip in consumer spending during 2020, reflecting the pandemic's impact, followed by a gradual recovery in 2021 and 2022, with spending becoming more stable over time."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cG_ppZOtHaR",
      "metadata": {
        "id": "3cG_ppZOtHaR"
      },
      "source": [
        "### Boxplots of Consumer Spending by Income Quartiles\n",
        "\n",
        "This section presents boxplots of consumer spending categorized by income quartiles, offering a visual comparison of spending distributions across different income groups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7lOkz08mZxnS",
      "metadata": {
        "id": "7lOkz08mZxnS"
      },
      "outputs": [],
      "source": [
        "# Filter quartile columns\n",
        "quartile_columns = ['spend_all_q1', 'spend_all_q2', 'spend_all_q3', 'spend_all_q4']\n",
        "\n",
        "# Ensure quartile columns are numeric\n",
        "for column in quartile_columns:\n",
        "    affinity_data[column] = pd.to_numeric(affinity_data[column], errors='coerce')\n",
        "\n",
        "# Drop rows where all quartile columns are NaN\n",
        "affinity_data = affinity_data.dropna(subset=quartile_columns, how='all')\n",
        "\n",
        "# Melt the data for easier plotting with Seaborn\n",
        "melted_data = affinity_data[quartile_columns].melt(var_name='Income Quartile', value_name='Spending')\n",
        "\n",
        "# Create a boxplot using Seaborn with the same palette\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=melted_data, x='Income Quartile', y='Spending', hue='Income Quartile', palette='coolwarm', dodge=False, legend=False)\n",
        "plt.title('Boxplot of Consumer Spending by Income Quartiles')\n",
        "plt.xlabel('Income Quartiles')\n",
        "plt.ylabel('Spending')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2n9ct2SfAmm",
      "metadata": {
        "id": "c2n9ct2SfAmm"
      },
      "source": [
        "The boxplot shows that consumer spending increases with income quartiles, with higher median spending and wider variability in the upper quartiles, while lower quartiles exhibit more negative outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing, resampling, indexing and cleanup\n",
        "\n",
        "This section outlines the steps taken to preprocess, resample, and clean the dataset, including linking related variables, handling missing values, and ensuring the data is ready for analysis and modeling."
      ],
      "metadata": {
        "id": "K0q7hvFfGArM"
      },
      "id": "K0q7hvFfGArM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Yh1c6y5gdQNJ",
      "metadata": {
        "id": "Yh1c6y5gdQNJ"
      },
      "outputs": [],
      "source": [
        "# Load Employment Data\n",
        "employment_data_path = f\"{data_dir}/Employment - National - Weekly.csv\"\n",
        "employment_data = pd.read_csv(employment_data_path)\n",
        "\n",
        "# Load UI Claims Data\n",
        "ui_claims_data_path = f\"{data_dir}/UI Claims - National - Weekly.csv\"\n",
        "ui_claims_data = pd.read_csv(ui_claims_data_path)\n",
        "\n",
        "# Inspect columns in employment data\n",
        "print(employment_data.columns)\n",
        "\n",
        "# Inspect columns in UI claims data\n",
        "print(ui_claims_data.columns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MASovzmseOVk",
      "metadata": {
        "id": "MASovzmseOVk"
      },
      "source": [
        "The `Affinity` data is daily, while the `employment_data` and `ui_claims_data` are weekly, based on the `day_endofweek` field. To merge these datasets meaningfully, we need to aggregate the daily Affinity data into weekly data, ensuring consistency in time periods.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pfcJRvNjfflR",
      "metadata": {
        "id": "pfcJRvNjfflR"
      },
      "outputs": [],
      "source": [
        "# Ensure the date column is set as the index\n",
        "affinity_data['date'] = pd.to_datetime(affinity_data[['year', 'month', 'day']])\n",
        "affinity_data.set_index('date', inplace=True)\n",
        "\n",
        "# Resample daily data to weekly\n",
        "weekly_affinity_data = affinity_data.resample('W-SAT').sum()\n",
        "\n",
        "# Verify the resampled data\n",
        "print(weekly_affinity_data.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72iIrSDtgUkK",
      "metadata": {
        "id": "72iIrSDtgUkK"
      },
      "outputs": [],
      "source": [
        "# Reset the index to create a 'date' column after resampling\n",
        "weekly_affinity_data = weekly_affinity_data.reset_index()\n",
        "\n",
        "# Verify the columns in weekly_affinity_data\n",
        "print(weekly_affinity_data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L9JpW0R6g_cL",
      "metadata": {
        "id": "L9JpW0R6g_cL"
      },
      "outputs": [],
      "source": [
        "# Inspect the values in day_endofweek\n",
        "print(employment_data['day_endofweek'].unique())\n",
        "print(ui_claims_data['day_endofweek'].unique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9XpBESJUghfO",
      "metadata": {
        "id": "9XpBESJUghfO"
      },
      "outputs": [],
      "source": [
        "# Helper function to correct invalid day values\n",
        "def correct_invalid_dates(df):\n",
        "    # Create a 'date' column with error handling\n",
        "    df['date'] = pd.to_datetime(\n",
        "        df[['year', 'month', 'day_endofweek']].rename(columns={'day_endofweek': 'day'}),\n",
        "        errors='coerce'  # Invalid dates will be set to NaT\n",
        "    )\n",
        "    # Fill NaT values by capping 'day_endofweek' to the last valid day of the month\n",
        "    invalid_dates = df['date'].isna()\n",
        "    if invalid_dates.any():\n",
        "        df.loc[invalid_dates, 'day_endofweek'] = df.loc[invalid_dates].apply(\n",
        "            lambda row: pd.Timestamp(f\"{row['year']}-{row['month']}-01\").days_in_month,\n",
        "            axis=1\n",
        "        )\n",
        "        # Recreate the corrected date\n",
        "        df['date'] = pd.to_datetime(\n",
        "            df[['year', 'month', 'day_endofweek']].rename(columns={'day_endofweek': 'day'})\n",
        "        )\n",
        "    return df\n",
        "\n",
        "# Correct invalid dates in employment_data and ui_claims_data\n",
        "employment_data = correct_invalid_dates(employment_data)\n",
        "ui_claims_data = correct_invalid_dates(ui_claims_data)\n",
        "\n",
        "# Verify the resulting date column\n",
        "print(employment_data[['year', 'month', 'day_endofweek', 'date']].head())\n",
        "print(ui_claims_data[['year', 'month', 'day_endofweek', 'date']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Yf1j5QeRgaLn",
      "metadata": {
        "id": "Yf1j5QeRgaLn"
      },
      "outputs": [],
      "source": [
        "# Check if 'date' exists in employment_data and ui_claims_data\n",
        "print(\"Employment Data Columns:\", employment_data.columns)\n",
        "print(\"UI Claims Data Columns:\", ui_claims_data.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "itoxHulKjfDH",
      "metadata": {
        "id": "itoxHulKjfDH"
      },
      "outputs": [],
      "source": [
        "# Select columns for correlation analysis\n",
        "correlation_columns = [\n",
        "    'spend_all', 'spend_all_q1', 'spend_all_q2', 'spend_all_q3', 'spend_all_q4',\n",
        "    'emp',  # Employment rate\n",
        "    'initclaims_count_regular'  # Unemployment claims\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "od8BKLNhkcYs",
      "metadata": {
        "id": "od8BKLNhkcYs"
      },
      "outputs": [],
      "source": [
        "# Find alignment on datasets weekly data\n",
        "print(weekly_affinity_data['date'].head())\n",
        "print(employment_data['date'].head())\n",
        "print(ui_claims_data['date'].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L8OycZM0kxOO",
      "metadata": {
        "id": "L8OycZM0kxOO"
      },
      "outputs": [],
      "source": [
        "# Align dates to Sundays for all datasets\n",
        "weekly_affinity_data['date'] = weekly_affinity_data['date'] + pd.offsets.Week(weekday=6)\n",
        "employment_data['date'] = employment_data['date'] + pd.offsets.Week(weekday=6)\n",
        "ui_claims_data['date'] = ui_claims_data['date'] + pd.offsets.Week(weekday=6)\n",
        "\n",
        "# Verify the new dates\n",
        "print(weekly_affinity_data['date'].head())\n",
        "print(employment_data['date'].head())\n",
        "print(ui_claims_data['date'].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83-Yq1Llk0Ud",
      "metadata": {
        "id": "83-Yq1Llk0Ud"
      },
      "outputs": [],
      "source": [
        "# Merge datasets\n",
        "merged_weekly_data = (\n",
        "    weekly_affinity_data\n",
        "    .merge(employment_data[['date', 'emp']], on='date', how='inner')\n",
        "    .merge(ui_claims_data[['date', 'initclaims_count_regular']], on='date', how='inner')\n",
        ")\n",
        "\n",
        "print(merged_weekly_data.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iLoMzhiqtTJp",
      "metadata": {
        "id": "iLoMzhiqtTJp"
      },
      "source": [
        "### Correlation Matrix of Spending and External Variables\n",
        "\n",
        "This section presents the correlation matrix to examine the relationships between consumer spending and external variables, providing insights into potential predictors for modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h5-uUv5haVQt",
      "metadata": {
        "id": "h5-uUv5haVQt"
      },
      "outputs": [],
      "source": [
        "# Fill missing values with the column mean\n",
        "merged_weekly_data[correlation_columns] = merged_weekly_data[correlation_columns].fillna(merged_weekly_data[correlation_columns].mean())\n",
        "\n",
        "# Compute correlation matrix\n",
        "cross_correlation_matrix = merged_weekly_data[correlation_columns].corr()\n",
        "\n",
        "# Plot correlation heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cross_correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', vmin=-1, vmax=1)\n",
        "plt.title('Correlation Matrix of Spending and External Variables')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hOjmXKkzllxe",
      "metadata": {
        "id": "hOjmXKkzllxe"
      },
      "source": [
        "The correlation matrix illustrates the relationships between consumer spending (total and by income quartiles), employment rates, and unemployment claims. Strong positive correlations are observed among the spending variables (`spend_all`, `spend_all_q1`, `spend_all_q2`, `spend_all_q3`, `spend_all_q4`), with values consistently above 0.9, indicating that spending patterns are closely aligned across income groups. Employment (`emp`) shows moderate positive correlations with spending (ranging from 0.62 to 0.77), suggesting that higher employment rates are associated with increased consumer spending. Conversely, unemployment claims (`initclaims_count_regular`) exhibit strong negative correlations with both spending and employment, with values as low as -0.84 for high-income spending (`spend_all_q4`). This indicates that rising unemployment claims correspond to declining spending and employment levels, highlighting the inverse relationship between economic downturns and consumer activity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pB8kySd2taU0",
      "metadata": {
        "id": "pB8kySd2taU0"
      },
      "source": [
        "### Pairplot of highly correlated (directly or inversely) variables\n",
        "\n",
        "This section uses pairplots to visualize the relationships between variables that show strong positive or negative correlations, helping to identify potential patterns and dependencies in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zIxIhngtnW1K",
      "metadata": {
        "id": "zIxIhngtnW1K"
      },
      "outputs": [],
      "source": [
        "# Select the columns of interest\n",
        "pairplot_columns = ['spend_all', 'emp', 'initclaims_count_regular']\n",
        "\n",
        "# Generate a pairplot\n",
        "sns.pairplot(merged_weekly_data[pairplot_columns])\n",
        "plt.suptitle('Pairplot of Spending, Employment, and Unemployment Claims', y=1.02)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GAzVjBS0odoE",
      "metadata": {
        "id": "GAzVjBS0odoE"
      },
      "source": [
        "This pairplot illustrates the interplay between consumer spending (`spend_all`), employment rate (`emp`), and unemployment claims (`initclaims_count_regular`). The diagonal plots highlight the distributions, with `spend_all` skewed toward higher values, and `initclaims_count_regular` displaying a heavy-tailed distribution indicative of a few outliers. The scatterplots reveal clear trends: consumer spending correlates positively with employment, reflecting increased economic activity during periods of higher employment, and negatively with unemployment claims, as higher claims indicate economic downturns. Similarly, employment is inversely correlated with unemployment claims, as expected. These relationships underscore the strong interdependencies among these economic indicators and set the stage for further analysis into their combined effects on broader economic trends.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "r4RyycLrauAO",
      "metadata": {
        "id": "r4RyycLrauAO"
      },
      "source": [
        "### Autocorrelation and Partial Autocorrelation Analysis of Residuals\n",
        "\n",
        "This section analyzes the residuals from the `spend_all` series using ACF and PACF plots. Residual analysis helps evaluate the adequacy of trend and seasonality removal, ensuring no significant autocorrelation remains. By examining the residuals from January 2020 to July 2024, this analysis identifies any remaining patterns or dependencies, which can indicate model misfit or the need for additional adjustments in the time series decomposition or modeling process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wtAvpq67XSWj",
      "metadata": {
        "id": "wtAvpq67XSWj"
      },
      "outputs": [],
      "source": [
        "# Use the residuals from the dataset\n",
        "residuals = affinity_data['residuals']\n",
        "# Drop missing values from residuals\n",
        "residuals = residuals.dropna()\n",
        "\n",
        "# Plot ACF and PACF for residuals\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(121)\n",
        "plot_acf(residuals, lags=50, ax=plt.gca(), title=\"ACF of Residuals\")\n",
        "plt.subplot(122)\n",
        "plot_pacf(residuals, lags=50, ax=plt.gca(), title=\"PACF of Residuals\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vs12BA_wYQE4",
      "metadata": {
        "id": "vs12BA_wYQE4"
      },
      "source": [
        "The ACF (Autocorrelation Function) plot indicates a strong correlation between residuals at shorter lags, gradually decaying as the lag increases. This suggests that the residuals exhibit significant autocorrelation, implying that the residuals are not purely random and might benefit from additional modeling to account for their structure.\n",
        "\n",
        "The PACF (Partial Autocorrelation Function) plot reveals strong correlations at initial lags, followed by a sharp drop-off. This suggests that the residuals are directly influenced by their recent past values, but the influence diminishes as we move further back in time. These findings indicate that a time-series model, such as an ARIMA model with a low-order autoregressive component, may effectively capture the remaining structure in the residuals.\n",
        "\n",
        "Both plots suggest that further analysis is necessary to refine the model and address the observed serial correlations, ensuring the residuals behave as a white noise process."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BgFD-URwbH-3",
      "metadata": {
        "id": "BgFD-URwbH-3"
      },
      "source": [
        "**Durbin-Watson statistic and Ljung-Box test**\n",
        "\n",
        "Expected Outcome:\n",
        "- Durbin-Watson Statistic: A number between 0 and 4, where:\n",
        " - ~2 indicates no autocorrelation.\n",
        " - \\<2 suggests positive autocorrelation.\n",
        " - \\>2 indicates negative autocorrelation.\n",
        "\n",
        "Ljung-Box Test: Provides lb_stat and lb_pvalue. A small p-value (\\<0.05) suggests significant autocorrelation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fWiBnoCX1aT",
      "metadata": {
        "id": "6fWiBnoCX1aT"
      },
      "outputs": [],
      "source": [
        "# Perform Durbin-Watson test\n",
        "dw_stat = durbin_watson(residuals)\n",
        "print(f\"Durbin-Watson Statistic: {dw_stat}\")\n",
        "\n",
        "# Perform Ljung-Box test\n",
        "ljungbox_result = acorr_ljungbox(residuals, lags=[10], return_df=True)\n",
        "print(ljungbox_result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iEIAI2FVaGJO",
      "metadata": {
        "id": "iEIAI2FVaGJO"
      },
      "source": [
        "The initial analysis of the residuals revealed significant positive autocorrelation, as shown by the low Durbin-Watson statistic and the results of the Ljung-Box test. This autocorrelation indicates that the residuals are not independent, which violates a key assumption of many statistical and time series models. To address this, we applied first-order differencing, a standard technique in time series analysis, to remove these dependencies and stabilize the series. By differencing, we essentially focused on the changes between consecutive observations, eliminating trends or patterns that could obscure the underlying relationships in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VuGCu4L_Z3_T",
      "metadata": {
        "id": "VuGCu4L_Z3_T"
      },
      "outputs": [],
      "source": [
        "# Apply first-order differencing\n",
        "residuals_diff = residuals.diff().dropna()\n",
        "\n",
        "# Recompute Durbin-Watson and Ljung-Box tests\n",
        "dw_stat_diff = durbin_watson(residuals_diff)\n",
        "lb_test_diff = acorr_ljungbox(residuals_diff, lags=[10], return_df=True)\n",
        "\n",
        "print(f\"Durbin-Watson Statistic after differencing: {dw_stat_diff}\")\n",
        "print(lb_test_diff)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rQ7HJTT6anXe",
      "metadata": {
        "id": "rQ7HJTT6anXe"
      },
      "source": [
        "After applying first-order differencing, the Durbin-Watson statistic improved significantly to 2.686, which is close to the ideal value of 2, suggesting that the residuals no longer exhibit strong positive autocorrelation. While the Ljung-Box test still detected minor autocorrelation, the differencing step mitigated most of the systematic dependencies, leaving the series more suitable for predictive modeling. Based on these results, we will proceed with an ARIMAX model, which incorporates autoregressive components and accounts for exogenous variables like employment and unemployment claims. This approach will allow us to capture the residual dependencies and the influence of external economic factors, providing a robust framework for analyzing and forecasting consumer spending trends."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Segmented Time Series Analysis\n",
        "\n",
        "To account for changes in spending behavior due to external events, we segment the data by year:\n",
        "\n",
        " - 2020: Pandemic onset and lockdowns.\n",
        " - 2021: Recovery and work-from-home adaptation.\n",
        " - 2022: Return to office, public events, and normalization.\n",
        "\n",
        "For each year, we analyze:\n",
        "\n",
        " 1. Total Spending (`spend_all`) - Primary series.\n",
        " 2. In-Person Spending (`spend_inperson`) - Exogenous variable.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NWWMJaWOOa9Z"
      },
      "id": "NWWMJaWOOa9Z"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to analyze ACF and PACF for a given series\n",
        "def plot_acf_pacf(series, title_prefix):\n",
        "    diff_series = series.diff().dropna()\n",
        "    seasonal_diff_series = diff_series.diff(12).dropna()\n",
        "\n",
        "    plt.figure(figsize=(14, 10))\n",
        "\n",
        "    # Original series\n",
        "    plt.subplot(321)\n",
        "    plot_acf(series, lags=30, ax=plt.gca(), title=f\"{title_prefix} - ACF (Original Series)\")\n",
        "    plt.subplot(322)\n",
        "    plot_pacf(series, lags=30, ax=plt.gca(), title=f\"{title_prefix} - PACF (Original Series)\")\n",
        "\n",
        "    # Differenced series\n",
        "    plt.subplot(323)\n",
        "    plot_acf(diff_series, lags=30, ax=plt.gca(), title=f\"{title_prefix} - ACF (Differenced Series)\")\n",
        "    plt.subplot(324)\n",
        "    plot_pacf(diff_series, lags=30, ax=plt.gca(), title=f\"{title_prefix} - PACF (Differenced Series)\")\n",
        "\n",
        "    # Seasonal differenced series\n",
        "    plt.subplot(325)\n",
        "    plot_acf(seasonal_diff_series, lags=30, ax=plt.gca(), title=f\"{title_prefix} - ACF (Seasonal Differenced)\")\n",
        "    plt.subplot(326)\n",
        "    plot_pacf(seasonal_diff_series, lags=30, ax=plt.gca(), title=f\"{title_prefix} - PACF (Seasonal Differenced)\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "DseybRQCObXI"
      },
      "id": "DseybRQCObXI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure all relevant columns are numeric\n",
        "affinity_data['spend_all'] = pd.to_numeric(affinity_data['spend_all'], errors='coerce')\n",
        "affinity_data['spend_inperson'] = pd.to_numeric(affinity_data['spend_inperson'], errors='coerce')\n",
        "\n",
        "# Filter data for 2020, 2021, and 2022 only\n",
        "affinity_data = affinity_data[affinity_data.index.year > 2019]\n",
        "\n",
        "# Segment the data into years and analyze\n",
        "for year in [2020, 2021, 2022]:\n",
        "    print(f\"\\n=== Analysis for {year} ===\")\n",
        "    year_data = affinity_data[affinity_data.index.year == year]\n",
        "\n",
        "    # Analyze the primary series (spend_all) and exogenous series (spend_inperson)\n",
        "    if not year_data['spend_all'].dropna().empty:\n",
        "        plot_acf_pacf(year_data['spend_all'].dropna(), f\"Total Spending ({year})\")\n",
        "    if not year_data['spend_inperson'].dropna().empty:\n",
        "        plot_acf_pacf(year_data['spend_inperson'].dropna(), f\"In-Person Spending ({year})\")\n"
      ],
      "metadata": {
        "id": "kG0xieWHT1p_"
      },
      "id": "kG0xieWHT1p_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Analysis of Segmented ACF and PACF Insights**\n",
        "\n",
        "This section highlights the key observations from the ACF and PACF plots of `spend_all` and `spend_inperson`. These insights guide the modeling process by identifying trends, short-term autocorrelation, and seasonal patterns. By analyzing the original, differenced, and seasonally differenced series, we assess the data's stationarity and determine the appropriate components for ARIMA and SARIMA models.\n",
        "\n"
      ],
      "metadata": {
        "id": "E8_86bawVolq"
      },
      "id": "E8_86bawVolq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Original Series\n",
        "   - **ACF (Autocorrelation Function):**\n",
        "     - The ACF for both `spend_all` and `spend_inperson` shows a **strong persistence** (slow decay) over lags.\n",
        "     - This indicates a clear trend in the data, meaning the series are non-stationary.\n",
        "     - The slow decay confirms the need for differencing to remove this trend.\n",
        "\n",
        "   - **PACF (Partial Autocorrelation Function):**\n",
        "     - The PACF shows a **large spike at lag 1** for all years, suggesting that an **AR(1)** model might be appropriate after differencing.\n",
        "     - This spike indicates that the series can be explained by its most recent value (lag 1).\n",
        "\n"
      ],
      "metadata": {
        "id": "bQzX_885LwBR"
      },
      "id": "bQzX_885LwBR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Differenced Series\n",
        "   - **ACF:**\n",
        "     - After applying first differencing, the ACF for `spend_all` and `spend_inperson` shows much faster decay, indicating that the trend has been successfully removed.\n",
        "     - The remaining significant spikes at early lags suggest some short-term autocorrelation.\n",
        "\n",
        "   - **PACF:**\n",
        "     - For the differenced series, the PACF shows significant spikes at **lag 1** and potentially at lag 2.\n",
        "     - This suggests that an **ARIMA(1,1,0)** or **ARIMA(2,1,0)** model could be appropriate for capturing the short-term autocorrelation after differencing."
      ],
      "metadata": {
        "id": "j2aScTLtMUxe"
      },
      "id": "j2aScTLtMUxe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seasonally Differenced Series (12-Month Lag):**\n",
        "   - **ACF:**\n",
        "     - Seasonal differencing removes some of the periodic patterns seen in the original series.\n",
        "     - The ACF for the seasonally differenced series still shows small spikes at lags 12, 24, etc., indicating **seasonality** in the data.\n",
        "\n",
        "   - **PACF:**\n",
        "     - The PACF for the seasonally differenced series has small spikes at lag 1 and sometimes at lag 12.\n",
        "     - This suggests that the seasonal behavior is less pronounced after differencing but still needs to be considered in the model."
      ],
      "metadata": {
        "id": "iuVAnTOrMURB"
      },
      "id": "iuVAnTOrMURB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Comparison Across Years**\n",
        "\n",
        "- **2020 (_Pandemic Onset_):**\n",
        "   - The ACF shows strong autocorrelation, reflecting the **sharp economic shock** and sustained impact.\n",
        "   - Differencing effectively removes the trend, leaving some short-term autocorrelation.\n",
        "\n",
        "- **2021 (_Recovery Phase_):**\n",
        "   - The trend is still visible in the original series, but differencing stabilizes the series.\n",
        "   - Seasonal effects are more visible in 2021 compared to 2020, likely due to changes in spending patterns as restrictions eased.\n",
        "\n",
        "- **2022 (_Return to Normalcy_):**\n",
        "   - The ACF and PACF patterns for 2022 are more stable compared to previous years.\n",
        "   - Differencing removes the trend effectively, and seasonal effects remain relatively minor.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JUATwE89XgUg"
      },
      "id": "JUATwE89XgUg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Implications for Modeling**\n",
        "\n",
        "The analysis of ACF and PACF reveals that differencing is essential to stabilize the series and remove underlying trends, ensuring the data is suitable for time series modeling. The patterns observed suggest that an ARIMA(1,1,0) or ARIMA(2,1,0) model would effectively capture short-term autocorrelation in the data. Furthermore, the seasonal effects observed at a lag of 12 highlight the importance of incorporating a seasonal component, necessitating the use of a SARIMA model to account for annual periodicity.\n",
        "\n",
        "Moving forward, ARIMA and SARIMA models will be implemented for each year using the differenced series to ensure proper handling of trends and seasonality. Additionally, the spend_inperson variable will be incorporated as an exogenous variable, after undergoing similar preprocessing, to capture its influence on total spending.\n"
      ],
      "metadata": {
        "id": "29aLVLY7XjEW"
      },
      "id": "29aLVLY7XjEW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Modeling**\n",
        "\n",
        "1. **Objective**:\n",
        "   - Build ARIMA/SARIMA models for each year (2020, 2021, 2022) using the primary series (`spend_all`).\n",
        "   - Include the exogenous variable (`spend_inperson`) for better forecasting.\n",
        "\n",
        "2. **Steps**:\n",
        "   - Use the differenced and seasonally differenced series for modeling.\n",
        "   - Apply the **SARIMAX** model, which handles both seasonal and non-seasonal components while incorporating exogenous variables.\n",
        "   - Evaluate model fit using metrics like **AIC** and residual diagnostics.\n"
      ],
      "metadata": {
        "id": "HRtereVnyfpv"
      },
      "id": "HRtereVnyfpv"
    },
    {
      "cell_type": "markdown",
      "id": "U9yi3l4Ib4uV",
      "metadata": {
        "id": "U9yi3l4Ib4uV"
      },
      "source": [
        "### SARIMAX Models\n",
        "\n",
        "The **SARIMAX (Seasonal AutoRegressive Integrated Moving Average with eXogenous variables)** model extends the ARIMA framework by incorporating seasonal components and external explanatory variables (e.g., **in-person spending**). This enables the model to capture:\n",
        "- **Internal patterns** in the target variable (`spend_all`) through autoregressive and moving average components.\n",
        "- **Seasonal effects**, such as periodic fluctuations in spending behavior.\n",
        "- **Influence of external factors**, like in-person spending, which reflects consumer behavior and economic trends.\n",
        "\n",
        "By integrating these components, the SARIMAX model offers a more comprehensive and accurate framework for understanding and forecasting changes in consumer spending. Below is the code to fit and evaluate the SARIMAX model for each year in the dataset (2020, 2021, 2022). The model diagnostics and residual analysis are also included to assess model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HPhukwmIcEJK",
      "metadata": {
        "id": "HPhukwmIcEJK"
      },
      "outputs": [],
      "source": [
        "# Define a function for SARIMAX modeling and diagnostics\n",
        "def fit_sarimax(train_series, exog_series, order, seasonal_order, year):\n",
        "    print(f\"\\n=== SARIMAX Model for {year} ===\")\n",
        "\n",
        "    # Fit the SARIMAX model\n",
        "    model = SARIMAX(\n",
        "        train_series,\n",
        "        exog=exog_series,\n",
        "        order=order,\n",
        "        seasonal_order=seasonal_order,\n",
        "        enforce_stationarity=False,\n",
        "        enforce_invertibility=False\n",
        "    )\n",
        "    results = model.fit(disp=False)\n",
        "\n",
        "    # Display model summary\n",
        "    print(results.summary())\n",
        "\n",
        "    # Plot diagnostics\n",
        "    results.plot_diagnostics(figsize=(14, 8))\n",
        "    plt.suptitle(f\"Model Diagnostics for {year}\", y=1.02, fontsize=16)\n",
        "    plt.show()\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SARIMAX model's non-seasonal (`order`) and seasonal (`seasonal_order`) parameters for each year are commom, and meant to capture short-term dependencies and annual seasonal patterns in the data."
      ],
      "metadata": {
        "id": "NNg3mCFT0pj5"
      },
      "id": "NNg3mCFT0pj5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Model parameters\n",
        "order = (1, 1, 0)  # ARIMA(p, d, q) for non-seasonal component\n",
        "seasonal_order = (1, 1, 0, 12)  # SARIMA(P, D, Q, S) for seasonal component"
      ],
      "metadata": {
        "id": "evnTk4Aw0CDN"
      },
      "id": "evnTk4Aw0CDN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Common function to fit models for 2020, 2021, and 2022\n",
        "def fit_sarimax_model(year):\n",
        "    # Filter data for the year\n",
        "    year_data = affinity_data[affinity_data.index.year == year]\n",
        "\n",
        "    # Extract primary series and exogenous series\n",
        "    train_series = year_data['spend_all'].dropna()\n",
        "    exog_series = year_data['spend_inperson'].dropna()\n",
        "\n",
        "    # Align exogenous series with the primary series\n",
        "    exog_series = exog_series.loc[train_series.index]\n",
        "\n",
        "    # Fit SARIMAX model\n",
        "    results = fit_sarimax(\n",
        "        train_series,\n",
        "        exog_series,\n",
        "        order=order,\n",
        "        seasonal_order=seasonal_order,\n",
        "        year=year\n",
        "    )\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "U4ldLr7L1eMT"
      },
      "id": "U4ldLr7L1eMT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **SARIMAX Model for 2020**"
      ],
      "metadata": {
        "id": "8lb0cfr5zpo0"
      },
      "id": "8lb0cfr5zpo0"
    },
    {
      "cell_type": "code",
      "source": [
        "results_2020 = fit_sarimax_model(2020);"
      ],
      "metadata": {
        "id": "X6N_jWUizpfY"
      },
      "id": "X6N_jWUizpfY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Results**\n",
        "- **Log Likelihood and AIC**:\n",
        "   - The model achieved a **log-likelihood of 1152.358** and an **AIC of -2296.716**, indicating a good fit for the dataset.\n",
        "\n",
        "- **Coefficients**:\n",
        "   - **In-person spending (`spend_inperson`)**: Strong positive relationship (**coef: 0.8137**, **p < 0.001**) with total spending. This means an increase in in-person spending strongly predicts an increase in total spending.\n",
        "   - **AR(1) (`ar.L1`)**: Significant positive coefficient (**coef: 0.3990**, **p < 0.001**) indicates that total spending is influenced by its immediate previous value.\n",
        "   - **Seasonal AR(12) (`ar.S.L12`)**: Significant negative coefficient (**coef: -0.5816**, **p < 0.001**) suggests a seasonal dependency, with spending 12 months prior having an inverse relationship with current spending.\n",
        "\n",
        "- **Residual Variance (`sigma2`)**:\n",
        "   - The residual variance is very low (**5.19e-05**), indicating the model captures most of the variability in the data."
      ],
      "metadata": {
        "id": "0IU2FjQQAhSi"
      },
      "id": "0IU2FjQQAhSi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Diagnostics**\n",
        "\n",
        "- Standardized Residuals: they appear centered around zero with no visible patterns, suggesting the model is well-specified.\n",
        "\n",
        "- Histogram and Q-Q Plot: Residuals follow a roughly normal distribution, though there are slight deviations in the tails. This is common with real-world financial data.\n",
        "\n",
        "- Correlogram: No significant autocorrelation in the residuals, confirming that the model successfully accounts for dependencies in the data."
      ],
      "metadata": {
        "id": "_z9DrGmh8Qk4"
      },
      "id": "_z9DrGmh8Qk4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SARIMAX model for 2020 performs well, capturing both short-term and seasonal dependencies in total spending while incorporating the impact of in-person spending. The significant coefficients and well-behaved residuals suggest that the model is robust and suitable for forecasting during this period. However, slight non-normality in residuals could be explored further if needed."
      ],
      "metadata": {
        "id": "2vPb2R6b_zGy"
      },
      "id": "2vPb2R6b_zGy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **SARIMAX Model for 2021**"
      ],
      "metadata": {
        "id": "jsiOiQISzpOu"
      },
      "id": "jsiOiQISzpOu"
    },
    {
      "cell_type": "code",
      "source": [
        "results_2021 = fit_sarimax_model(2021);"
      ],
      "metadata": {
        "id": "-xCRNrlTzoy_"
      },
      "id": "-xCRNrlTzoy_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Results**\n",
        "\n",
        "- **Log Likelihood and AIC**:\n",
        "   - The model achieved a **log-likelihood of 1181.704** and an **AIC of -2355.407**, indicating a good fit.\n",
        "\n",
        "- **Coefficients**:\n",
        "   - **In-person spending (`spend_inperson`)**: Strong positive coefficient (**coef: 0.8957**, **p < 0.001**) indicates that in-person spending is a highly significant predictor of total spending, with nearly a one-to-one relationship.\n",
        "   - **AR(1) (`ar.L1`)**: Positive coefficient (**coef: 0.2277**, **p < 0.001**) shows that total spending is influenced by its value in the previous period.\n",
        "   - **Seasonal AR(12) (`ar.S.L12`)**: Significant negative coefficient (**coef: -0.4894**, **p < 0.001**) reflects an inverse relationship with spending from 12 months prior.\n",
        "\n",
        "- **Residual Variance (`sigma2`)**:\n",
        "   - Low variance (**5.487e-05**) suggests that the model explains most of the variability in the data."
      ],
      "metadata": {
        "id": "8RxxYhNBzoiz"
      },
      "id": "8RxxYhNBzoiz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Diagnostics**\n",
        "\n",
        "- Standardized Residuals: appear well-centered around zero with no visible patterns, indicating the model is well-specified.\n",
        "\n",
        "- Histogram and Q-Q Plot: Residuals follow a roughly normal distribution, with slight deviations in the tails. These deviations may be due to some outlier spending patterns during the recovery phase.\n",
        "\n",
        "- Correlogram: No significant autocorrelation is observed in the residuals, confirming that the model successfully captures the dependencies in the data.\n"
      ],
      "metadata": {
        "id": "qmTUwVFS8Spw"
      },
      "id": "qmTUwVFS8Spw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SARIMAX model for 2021 performs well in capturing total spending trends during the recovery period. The strong correlation between in-person spending and total spending aligns with the gradual return to pre-pandemic behavior. The seasonal autoregressive term further reflects changes in consumer patterns compared to the same period in 2020. Residuals indicate a good model fit, although slight non-normality may require further exploration if higher precision is needed."
      ],
      "metadata": {
        "id": "cJlNWlcbAnwc"
      },
      "id": "cJlNWlcbAnwc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **SARIMAX Model for 2022**"
      ],
      "metadata": {
        "id": "bVrvbsMIMqDR"
      },
      "id": "bVrvbsMIMqDR"
    },
    {
      "cell_type": "code",
      "source": [
        "results_2022 = fit_sarimax_model(2022);"
      ],
      "metadata": {
        "id": "6Ak6gW8Pzqhw"
      },
      "id": "6Ak6gW8Pzqhw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Results**\n",
        "\n",
        "- **Log Likelihood and AIC**:\n",
        "   - The model achieved a **log-likelihood of 452.995** and an **AIC of -897.990**, suggesting the model provides a reasonable fit to the data.\n",
        "\n",
        "- **Coefficients**:\n",
        "   - **In-person spending (`spend_inperson`)**: Positive coefficient (**coef: 0.5389**, **p < 0.001**) shows a moderate relationship between in-person spending and total spending. This relationship is less pronounced than in previous years, reflecting potentially diversified spending behavior.\n",
        "   - **AR(1) (`ar.L1`)**: The coefficient is not significant (**p = 0.236**), indicating that immediate past values of total spending may not strongly influence current values.\n",
        "   - **Seasonal AR(12) (`ar.S.L12`)**: Significant negative coefficient (**coef: -0.3752**, **p = 0.006**) suggests that total spending has an inverse seasonal relationship with spending 12 months prior.\n",
        "\n",
        "- **Residual Variance (`sigma2`)**:\n",
        "   - The variance is slightly higher than in 2021 but still low (**0.0002**), indicating the model explains much of the variability."
      ],
      "metadata": {
        "id": "HD78ZLek8WlP"
      },
      "id": "HD78ZLek8WlP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Diagnostics**\n",
        "\n",
        "- Standardized Residuals: are generally centered around zero but exhibit one large spike toward the end of the year, which may indicate a significant, unaccounted event in the data.\n",
        "\n",
        "- Histogram and Q-Q Plot: Residuals mostly follow a normal distribution, though skewness is evident. This skewness aligns with outlier behavior, possibly due to economic volatility in late 2022.\n",
        "\n",
        "- Correlogram: Residual autocorrelation is minimal, confirming the model captures dependencies effectively.\n",
        "\n",
        "- Jarque-Bera Test: This is where 2022 differs from the previous 2 years. The very high **JB statistic $(14926.47)$** and low p-value $(p < 0.001)$ indicate that the residuals deviate significantly from normality, likely due to *outliers* and *heteroskedasticity*`."
      ],
      "metadata": {
        "id": "g7OvG87OBd4S"
      },
      "id": "g7OvG87OBd4S"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SARIMAX model for 2022 performs reasonably well in modeling total spending, capturing seasonal dependencies and the impact of in-person spending. However:\n",
        "- The influence of **in-person spending** is weaker compared to 2021, likely reflecting changes in consumer habits and spending distribution.\n",
        "- Residual diagnostics highlight potential outliers and volatility in late 2022, suggesting room for improvement in handling these anomalies."
      ],
      "metadata": {
        "id": "8HwC6cAUBdpG"
      },
      "id": "8HwC6cAUBdpG"
    },
    {
      "cell_type": "markdown",
      "id": "pbU34HLRdUgw",
      "metadata": {
        "id": "pbU34HLRdUgw"
      },
      "source": [
        "**Trying to address the index issue and enable forecasting**\n",
        "\n",
        "This ARIMAX model attempts to capture how changes in consumer spending are influenced by employment and unemployment claims, while accounting for temporal patterns. Although the model shows promising relationships, further refinement may be necessary to address diagnostics and improve reliability for forecasting and inference."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mYxeSOcgQOh0"
      },
      "id": "mYxeSOcgQOh0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function for forecasting and evaluation for a given year\n",
        "def forecast_for_december(year, model_results):\n",
        "    print(f\"\\n=== Forecasting for December {year} ===\")\n",
        "\n",
        "    # Define the date range for December\n",
        "    forecast_start_date = f\"{year}-12-01\"\n",
        "    forecast_end_date = f\"{year}-12-31\"\n",
        "\n",
        "    # Prepare exogenous and actual data for the forecast period\n",
        "    exog_forecast = affinity_data.loc[forecast_start_date:forecast_end_date, 'spend_inperson'].dropna()\n",
        "    actuals = affinity_data.loc[forecast_start_date:forecast_end_date, 'spend_all'].dropna()\n",
        "\n",
        "    # Check if there is enough data to proceed\n",
        "    if exog_forecast.empty or actuals.empty:\n",
        "        print(f\"Not enough data to forecast for December {year}. Skipping.\")\n",
        "        return None\n",
        "\n",
        "    # Generate forecast\n",
        "    forecast = model_results.get_forecast(steps=len(actuals), exog=exog_forecast)\n",
        "    forecast_values = forecast.predicted_mean\n",
        "    conf_int = forecast.conf_int()\n",
        "\n",
        "    # Evaluate forecast performance\n",
        "    mae = mean_absolute_error(actuals, forecast_values)\n",
        "    rmse = np.sqrt(mean_squared_error(actuals, forecast_values))\n",
        "\n",
        "    print(f\"MAE: {mae:.4f}\")\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "\n",
        "    # Plot forecast versus actuals\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(actuals.index, actuals, label=\"Actual\", color=\"blue\")\n",
        "    plt.plot(actuals.index, forecast_values, label=\"Forecast\", color=\"red\")\n",
        "    plt.fill_between(\n",
        "        actuals.index,\n",
        "        conf_int.iloc[:, 0],\n",
        "        conf_int.iloc[:, 1],\n",
        "        color=\"pink\",\n",
        "        alpha=0.3,\n",
        "        label=\"Confidence Interval\"\n",
        "    )\n",
        "    plt.title(f\"SARIMAX Forecast vs Actual for December {year}\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return forecast_values\n"
      ],
      "metadata": {
        "id": "WztqpSVjQO7n"
      },
      "id": "WztqpSVjQO7n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "I0BknG9VU9sf"
      },
      "id": "I0BknG9VU9sf"
    },
    {
      "cell_type": "code",
      "source": [
        "# Forecast for December 2020\n",
        "forecast_values_2020 = forecast_for_december(2020, results_2020)\n"
      ],
      "metadata": {
        "id": "wV2wDYeSUXjq"
      },
      "id": "wV2wDYeSUXjq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forecast for December 2021\n",
        "forecast_values_2021 = forecast_for_december(2021, results_2021)\n"
      ],
      "metadata": {
        "id": "taRMt60RUXYH"
      },
      "id": "taRMt60RUXYH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forecast for December 2022\n",
        "forecast_values_2022 = forecast_for_december(2022, results_2022)\n"
      ],
      "metadata": {
        "id": "9atC5RJqUXMW"
      },
      "id": "9atC5RJqUXMW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e4fd1514",
      "metadata": {
        "id": "e4fd1514"
      },
      "source": [
        "# Conclusion and Next Steps\n",
        "\n",
        "The exploratory data analysis (EDA) conducted for this proposal has reaffirmed the immense potential of the Opportunity Insights Economic Tracker dataset in uncovering nuanced insights into economic behavior and its relationships with external factors. By focusing on consumer spending, employment rates, and unemployment claims, the analysis has provided a solid foundation for understanding the interconnectedness of these key economic indicators. The added steps of detrending, differencing, and residual analysis have further ensured that the statistical methodologies employed are robust and provide reliable insights.\n",
        "\n",
        "Beyond the work presented in this proposal, there remains significant scope for additional exploration. Factors such as the impact of COVID-19 lockdowns, hospitalization and death rates, public policies, government stimulus checks, mobility patterns, and sector-specific disparities offer rich avenues for further investigation. Incorporating scenario-based forecasting and feature-level analyses across income quartiles and industries will provide deeper insights into the economic disparities and recovery patterns observed during turbulent periods. Furthermore, the introduction of ARIMAX modeling has added a predictive dimension to the analysis, enabling the study of how external variables like employment and unemployment claims drive spending trends.\n",
        "\n",
        "The final project will build upon this proposal by incorporating the broader dimensions of the dataset, leveraging advanced statistical techniques such as residual diagnostics, serial correlation tests, and ARIMAX-based forecasting. Additionally, evaluation metrics, scenario-based simulations, and real-world event interpretations will enrich the analysis, ensuring a comprehensive and actionable understanding of economic resilience and policy impacts.\n",
        "\n",
        "Overall, the EDA has validated the dataset's relevance, granularity, and analytical richness, while highlighting its suitability for advanced modeling and forecasting. This proposal lays a strong foundation for the final project, ensuring it will be both rigorous and insightful, with significant practical implications for economic policy and decision-making."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vjjkgUSuq3Oc",
      "metadata": {
        "id": "vjjkgUSuq3Oc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}