{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXiigqlQF9Fm"
      },
      "source": [
        "# Chapter 29     \n",
        "# Bayesian Modeling and Markov Chain Monte Carlo\n",
        "\n",
        "**Student:** Luciano Carvalho\n",
        "\n",
        "## Overview\n",
        "\n",
        "In a previous lesson we explored the basics of Bayesian parameter estimation. The methods we used are restricted to only simple models. This lesson introduces you to a general and flexible form of Bayesian modeling using the **Markov chain Monte Carlo (MCMC)** methods. MCMC methods can be extended to extremely complex models, including **Bayesian hierarchical models**.  \n",
        "\n",
        "![](../images/Flips.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSTYAyIoF9Fn"
      },
      "source": [
        "## Software\n",
        "\n",
        "Most Bayes software packages use efficient Markov chain Monte Carlo (MCMC) methods. The most widely used of these is [Stan](https://mc-stan.org/), named for mathematician Stanislaw Ulam. Stan also includes variational approximation methods.\n",
        "\n",
        "A powerful, and generally more user friendly, Python package is [PyMC](https://docs.pymc.io/). The code in this notebook uses PyMC3. The documentation for PyMC3 includes an excellent [Getting Started Jupyter Notebook](https://docs.pymc.io/notebooks/getting_started.html) along with other tutorials. Additionally you can find a number of [example Jupyter notebooks](https://docs.pymc.io/nb_examples/index.html) for many application areas.  \n",
        "\n",
        "First, you need to make sure your environment is ready to run the code in this notebook. The following notes will help you do so. You will need to restart your Jupyter server for any newly installed packages to be loaded.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTe3mTXUF9Fn"
      },
      "source": [
        "***\n",
        "**Note 1::**  To execute the code in this notebook you will also need to install [ArviZ](https://arviz-devs.github.io/arviz/#) by following [these directions](https://arviz-devs.github.io/arviz/getting_started/Installation.html). Typically a Conda installation is preferred."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxgYHYK2F9Fn"
      },
      "source": [
        "***\n",
        "**Note 2:** To execute the code in this notebook you must have installed PyMC3 by following [these instructions](https://www.pymc.io/projects/docs/en/stable/installation.html). When you import PyMC, you may see a warning message, which tells you ` g++ not available`, and which will affect the computational performance of PyMC. In this case you need to run the following at a command prompt: `conda install m2w64-toolchain`.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWifEw4vF9Fn"
      },
      "source": [
        "***\n",
        "**Note 3:** Since the installation of PyMC can be difficult, you may chose to run this notebook in Google [Colaboratory](https://colab.research.google.com/). Colab has PyMC installed.\n",
        "***  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqdEERMtF9Fo"
      },
      "source": [
        "## Review of Bayes Theorem\n",
        "\n",
        "Recall Bayes theorem:\n",
        "\n",
        "$$P(A|B) = \\frac{P(A)P(B|A)}{P(B)}$$\n",
        "\n",
        "Computing the normalization $P(B)$ is a bit of a mess. But fortunately, we don't always need the denominator. We can rewrite Bayes Theorem as:\n",
        "\n",
        "$$ùëÉ(ùê¥‚îÇùêµ)=ùëò‚àôùëÉ(ùêµ|ùê¥)ùëÉ(ùê¥)$$\n",
        "\n",
        "Ignoring the normalizaton constant $k$, we get:\n",
        "\n",
        "$$ùëÉ(ùê¥‚îÇùêµ) \\propto ùëÉ(ùêµ|ùê¥)ùëÉ(ùê¥)$$\n",
        "\n",
        "### Bayesian parameter estimation\n",
        "\n",
        "How to we interpret the relationships shown above? We do this as follows:\n",
        "\n",
        "$$Posterior\\ Distribution \\propto Likelihood \\bullet Prior\\ Distribution \\\\\n",
        "Or\\\\\n",
        "ùëÉ(ùëùùëéùëüùëéùëöùëíùë°ùëíùëüùë†‚îÇùëëùëéùë°ùëé) \\propto ùëÉ(ùëëùëéùë°ùëé|ùëùùëéùëüùëéùëöùëíùë°ùëíùëüùë†)ùëÉ(ùëùùëéùëüùëéùëöùëíùë°ùëíùëüùë†) $$\n",
        "\n",
        "These relationships apply to the observed data distributions, or to parameters in a model (partial slopes, intercept, error distributions, lasso constant,‚Ä¶)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmtBa565F9Fo"
      },
      "source": [
        "## Grid Sampling and Scalability\n",
        "\n",
        "Real-world Bayes models have large numbers of parameters, even into the millions. As a naive approach to Bayesian analysis would be to simply grid sample across the dimensions of the parameter space. However, grid sampling will not scale. To underestand the scaling problem, do the following thought experiment, where each dimension is sampled 100 times:\n",
        "\n",
        "- For a 1-parameter model: $100$ samples.\n",
        "- For a 2-parameter model: $100^2 = 10000$ samples.\n",
        "- For a 3-parameter model: $100^3 = 10^5$ samples.\n",
        "- For a 100-parameter model: $100^{100} = 10^{102}$ samples.\n",
        "\n",
        "As you can see, the compuational complexity of grid sampling has **exponential scaling** with dimensionality. Clearly, we need a better approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocse67azF9Fo"
      },
      "source": [
        "## Introduction to Markov Chain Monte Carlo\n",
        "\n",
        "Large-scale Bayesian models use a family of efficient sampling methods known as **Markov chain Monte Carlo sampling**. Rather that systematically sampling on a grid MCMC methods sample distributions randomly. While MCMC methods are computationally efficient, but requires effort to understand how it works and what to do when things go wrong.\n",
        "\n",
        "Execute the code in the cell below to import the required packages.   \n",
        "\n",
        "> **Note:** The lastest version of PUMC, V 5, has migrated to the [PyTensor](https://pytensor.readthedocs.io/en/latest/) platform. PyTensor has specific version requirements in dependent packages, particularly Numpy. You may need to update your Anaconda environment by running the following from at a command prompt.\n",
        "  "
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "ilMlheWkF9Fo"
      },
      "source": [
        "conda update --all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7kY3Sc4F9Fp"
      },
      "source": [
        "> Make sure you have removed old versions of PYMC and PYMC3."
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "qdDaF1_PF9Fp"
      },
      "source": [
        "conda remove pymc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_dGy8r5F9Fp"
      },
      "source": [
        "> Now you can install a dependent package `cachetools` and finally, PYMC."
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "G-ouuN-nF9Fp"
      },
      "source": [
        "conda install cachetools, PYMC"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q condacolab\n"
      ],
      "metadata": {
        "id": "zS3V9hlgGi83"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import condacolab\n",
        "condacolab.install()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJIywGN5HGnm",
        "outputId": "0b2b45b7-0edc-4c0d-f06a-e2e8978ad62e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è¨ Downloading https://github.com/conda-forge/miniforge/releases/download/23.11.0-0/Mambaforge-23.11.0-0-Linux-x86_64.sh...\n",
            "üì¶ Installing...\n",
            "üìå Adjusting configuration...\n",
            "ü©π Patching environment...\n",
            "‚è≤ Done in 0:00:23\n",
            "üîÅ Restarting kernel...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda --version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gF4DewgKHT5P",
        "outputId": "55f8906d-d929-4477-86c6-e626201844e9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conda 23.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda update --all\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sny9xTO6HXWS",
        "outputId": "2324654f-c4c0-49f6-d914-05a38b904713"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Channels:\n",
            " - conda-forge\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\b\\ \b\b| \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    archspec-0.2.3             |     pyhd8ed1ab_0          48 KB  conda-forge\n",
            "    boltons-24.0.0             |     pyhd8ed1ab_0         291 KB  conda-forge\n",
            "    brotli-python-1.1.0        |  py310hf71b8c6_2         341 KB  conda-forge\n",
            "    bzip2-1.0.8                |       h4bc722e_7         247 KB  conda-forge\n",
            "    c-ares-1.34.3              |       hb9d3cd8_1         200 KB  conda-forge\n",
            "    ca-certificates-2024.8.30  |       hbcca054_0         155 KB  conda-forge\n",
            "    certifi-2024.8.30          |     pyhd8ed1ab_0         160 KB  conda-forge\n",
            "    cffi-1.17.1                |  py310h8deb56e_0         238 KB  conda-forge\n",
            "    charset-normalizer-3.4.0   |     pyhd8ed1ab_0          46 KB  conda-forge\n",
            "    conda-24.7.1               |  py310hff52083_0         940 KB  conda-forge\n",
            "    conda-libmamba-solver-24.9.0|     pyhd8ed1ab_0          41 KB  conda-forge\n",
            "    conda-package-handling-2.4.0|     pyh7900ff3_0         252 KB  conda-forge\n",
            "    conda-package-streaming-0.11.0|     pyhd8ed1ab_0          20 KB  conda-forge\n",
            "    distro-1.9.0               |     pyhd8ed1ab_0          41 KB  conda-forge\n",
            "    fmt-10.2.1                 |       h00ab1b0_0         189 KB  conda-forge\n",
            "    frozendict-2.4.6           |  py310ha75aee5_0          48 KB  conda-forge\n",
            "    h2-4.1.0                   |     pyhd8ed1ab_0          46 KB  conda-forge\n",
            "    hpack-4.0.0                |     pyh9f0ad1d_0          25 KB  conda-forge\n",
            "    hyperframe-6.0.1           |     pyhd8ed1ab_0          14 KB  conda-forge\n",
            "    idna-3.10                  |     pyhd8ed1ab_0          49 KB  conda-forge\n",
            "    jsonpointer-3.0.0          |  py310hff52083_1          15 KB  conda-forge\n",
            "    krb5-1.21.3                |       h659f571_0         1.3 MB  conda-forge\n",
            "    ld_impl_linux-64-2.43      |       h712a8e2_2         654 KB  conda-forge\n",
            "    libarchive-3.7.4           |       hfca40fe_0         851 KB  conda-forge\n",
            "    libcurl-8.8.0              |       hca28451_1         401 KB  conda-forge\n",
            "    libgcc-14.2.0              |       h77fa898_1         829 KB  conda-forge\n",
            "    libgcc-ng-14.2.0           |       h69a702a_1          53 KB  conda-forge\n",
            "    libgomp-14.2.0             |       h77fa898_1         450 KB  conda-forge\n",
            "    libmamba-1.5.8             |       had39da4_0         1.6 MB  conda-forge\n",
            "    libmambapy-1.5.8           |  py310h39ff949_0         302 KB  conda-forge\n",
            "    libsolv-0.7.29             |       ha6fb4c9_0         460 KB  conda-forge\n",
            "    libsqlite-3.46.0           |       hde9e2c9_0         845 KB  conda-forge\n",
            "    libstdcxx-14.2.0           |       hc0a3c3a_1         3.7 MB  conda-forge\n",
            "    libstdcxx-ng-14.2.0        |       h4852527_1          53 KB  conda-forge\n",
            "    libxml2-2.12.7             |       hc051c1a_1         688 KB  conda-forge\n",
            "    libzlib-1.2.13             |       h4ab18f5_6          60 KB  conda-forge\n",
            "    lzo-2.10                   |    hd590300_1001         167 KB  conda-forge\n",
            "    mamba-1.5.8                |  py310h51d5547_0          51 KB  conda-forge\n",
            "    menuinst-2.2.0             |  py310hff52083_0         137 KB  conda-forge\n",
            "    ncurses-6.5                |       he02047a_1         868 KB  conda-forge\n",
            "    openssl-3.4.0              |       hb9d3cd8_0         2.8 MB  conda-forge\n",
            "    packaging-24.2             |     pyhff2d567_1          59 KB  conda-forge\n",
            "    pip-24.3.1                 |     pyh8b19718_0         1.2 MB  conda-forge\n",
            "    platformdirs-4.3.6         |     pyhd8ed1ab_0          20 KB  conda-forge\n",
            "    pluggy-1.5.0               |     pyhd8ed1ab_0          23 KB  conda-forge\n",
            "    pycosat-0.6.6              |  py310ha75aee5_2          83 KB  conda-forge\n",
            "    pycparser-2.22             |     pyhd8ed1ab_0         103 KB  conda-forge\n",
            "    reproc-14.2.5.post0        |       hb9d3cd8_0          33 KB  conda-forge\n",
            "    reproc-cpp-14.2.5.post0    |       h5888daf_0          25 KB  conda-forge\n",
            "    requests-2.32.3            |     pyhd8ed1ab_0          57 KB  conda-forge\n",
            "    ruamel.yaml-0.18.6         |  py310ha75aee5_1         198 KB  conda-forge\n",
            "    ruamel.yaml.clib-0.2.8     |  py310ha75aee5_1         143 KB  conda-forge\n",
            "    setuptools-75.6.0          |     pyhff2d567_1         756 KB  conda-forge\n",
            "    tqdm-4.67.1                |     pyhd8ed1ab_0          87 KB  conda-forge\n",
            "    truststore-0.10.0          |     pyhd8ed1ab_0          21 KB  conda-forge\n",
            "    tzdata-2024b               |       hc8b5060_0         119 KB  conda-forge\n",
            "    urllib3-2.2.3              |     pyhd8ed1ab_0          96 KB  conda-forge\n",
            "    wheel-0.45.1               |     pyhd8ed1ab_0          62 KB  conda-forge\n",
            "    zstandard-0.23.0           |  py310ha39cb0e_1         399 KB  conda-forge\n",
            "    zstd-1.5.6                 |       ha6fb4c9_0         542 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        23.4 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  frozendict         conda-forge/linux-64::frozendict-2.4.6-py310ha75aee5_0 \n",
            "  h2                 conda-forge/noarch::h2-4.1.0-pyhd8ed1ab_0 \n",
            "  hpack              conda-forge/noarch::hpack-4.0.0-pyh9f0ad1d_0 \n",
            "  hyperframe         conda-forge/noarch::hyperframe-6.0.1-pyhd8ed1ab_0 \n",
            "  libgcc             conda-forge/linux-64::libgcc-14.2.0-h77fa898_1 \n",
            "  libstdcxx          conda-forge/linux-64::libstdcxx-14.2.0-hc0a3c3a_1 \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  archspec                               0.2.2-pyhd8ed1ab_0 --> 0.2.3-pyhd8ed1ab_0 \n",
            "  boltons                               23.1.1-pyhd8ed1ab_0 --> 24.0.0-pyhd8ed1ab_0 \n",
            "  brotli-python                       1.1.0-py310hc6cd4ac_1 --> 1.1.0-py310hf71b8c6_2 \n",
            "  bzip2                                    1.0.8-hd590300_5 --> 1.0.8-h4bc722e_7 \n",
            "  c-ares                                  1.24.0-hd590300_0 --> 1.34.3-hb9d3cd8_1 \n",
            "  ca-certificates                     2023.11.17-hbcca054_0 --> 2024.8.30-hbcca054_0 \n",
            "  certifi                           2023.11.17-pyhd8ed1ab_0 --> 2024.8.30-pyhd8ed1ab_0 \n",
            "  cffi                               1.16.0-py310h2fee648_0 --> 1.17.1-py310h8deb56e_0 \n",
            "  charset-normalizer                     3.3.2-pyhd8ed1ab_0 --> 3.4.0-pyhd8ed1ab_0 \n",
            "  conda                             23.11.0-py310hff52083_1 --> 24.7.1-py310hff52083_0 \n",
            "  conda-libmamba-so~                   23.12.0-pyhd8ed1ab_0 --> 24.9.0-pyhd8ed1ab_0 \n",
            "  conda-package-han~                     2.2.0-pyh38be061_0 --> 2.4.0-pyh7900ff3_0 \n",
            "  conda-package-str~                     0.9.0-pyhd8ed1ab_0 --> 0.11.0-pyhd8ed1ab_0 \n",
            "  distro                                 1.8.0-pyhd8ed1ab_0 --> 1.9.0-pyhd8ed1ab_0 \n",
            "  fmt                                     10.1.1-h00ab1b0_1 --> 10.2.1-h00ab1b0_0 \n",
            "  idna                                     3.6-pyhd8ed1ab_0 --> 3.10-pyhd8ed1ab_0 \n",
            "  jsonpointer                           2.4-py310hff52083_3 --> 3.0.0-py310hff52083_1 \n",
            "  krb5                                    1.21.2-h659d440_0 --> 1.21.3-h659f571_0 \n",
            "  ld_impl_linux-64                          2.40-h41732ed_0 --> 2.43-h712a8e2_2 \n",
            "  libarchive                               3.7.2-h2aa1ff5_1 --> 3.7.4-hfca40fe_0 \n",
            "  libcurl                                  8.5.0-hca28451_0 --> 8.8.0-hca28451_1 \n",
            "  libgcc-ng                               13.2.0-h807b86a_3 --> 14.2.0-h69a702a_1 \n",
            "  libgomp                                 13.2.0-h807b86a_3 --> 14.2.0-h77fa898_1 \n",
            "  libmamba                                 1.5.5-had39da4_0 --> 1.5.8-had39da4_0 \n",
            "  libmambapy                          1.5.5-py310h39ff949_0 --> 1.5.8-py310h39ff949_0 \n",
            "  libsolv                                 0.7.27-hfc55251_0 --> 0.7.29-ha6fb4c9_0 \n",
            "  libsqlite                               3.44.2-h2797004_0 --> 3.46.0-hde9e2c9_0 \n",
            "  libstdcxx-ng                            13.2.0-h7e041cc_3 --> 14.2.0-h4852527_1 \n",
            "  libxml2                                 2.12.3-h232c23b_0 --> 2.12.7-hc051c1a_1 \n",
            "  libzlib                                 1.2.13-hd590300_5 --> 1.2.13-h4ab18f5_6 \n",
            "  lzo                                    2.10-h516909a_1000 --> 2.10-hd590300_1001 \n",
            "  mamba                               1.5.5-py310h51d5547_0 --> 1.5.8-py310h51d5547_0 \n",
            "  menuinst                            2.0.1-py310hff52083_0 --> 2.2.0-py310hff52083_0 \n",
            "  ncurses                                    6.4-h59595ed_2 --> 6.5-he02047a_1 \n",
            "  openssl                                  3.2.0-hd590300_1 --> 3.4.0-hb9d3cd8_0 \n",
            "  packaging                               23.2-pyhd8ed1ab_0 --> 24.2-pyhff2d567_1 \n",
            "  pip                                   23.3.2-pyhd8ed1ab_0 --> 24.3.1-pyh8b19718_0 \n",
            "  platformdirs                           4.1.0-pyhd8ed1ab_0 --> 4.3.6-pyhd8ed1ab_0 \n",
            "  pluggy                                 1.3.0-pyhd8ed1ab_0 --> 1.5.0-pyhd8ed1ab_0 \n",
            "  pycosat                             0.6.6-py310h2372a71_0 --> 0.6.6-py310ha75aee5_2 \n",
            "  pycparser                               2.21-pyhd8ed1ab_0 --> 2.22-pyhd8ed1ab_0 \n",
            "  reproc                            14.2.4.post0-hd590300_1 --> 14.2.5.post0-hb9d3cd8_0 \n",
            "  reproc-cpp                        14.2.4.post0-h59595ed_1 --> 14.2.5.post0-h5888daf_0 \n",
            "  requests                              2.31.0-pyhd8ed1ab_0 --> 2.32.3-pyhd8ed1ab_0 \n",
            "  ruamel.yaml                        0.18.5-py310h2372a71_0 --> 0.18.6-py310ha75aee5_1 \n",
            "  ruamel.yaml.clib                    0.2.7-py310h2372a71_2 --> 0.2.8-py310ha75aee5_1 \n",
            "  setuptools                            68.2.2-pyhd8ed1ab_0 --> 75.6.0-pyhff2d567_1 \n",
            "  tqdm                                  4.66.1-pyhd8ed1ab_0 --> 4.67.1-pyhd8ed1ab_0 \n",
            "  truststore                             0.8.0-pyhd8ed1ab_0 --> 0.10.0-pyhd8ed1ab_0 \n",
            "  tzdata                                   2023c-h71feb2d_0 --> 2024b-hc8b5060_0 \n",
            "  urllib3                                2.1.0-pyhd8ed1ab_0 --> 2.2.3-pyhd8ed1ab_0 \n",
            "  wheel                                 0.42.0-pyhd8ed1ab_0 --> 0.45.1-pyhd8ed1ab_0 \n",
            "  zstandard                          0.22.0-py310h1275a96_0 --> 0.23.0-py310ha39cb0e_1 \n",
            "  zstd                                     1.5.5-hfc55251_0 --> 1.5.6-ha6fb4c9_0 \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "libstdcxx-14.2.0     | 3.7 MB    | :   0% 0/1 [00:00<?, ?it/s]\n",
            "openssl-3.4.0        | 2.8 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "libmamba-1.5.8       | 1.6 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "krb5-1.21.3          | 1.3 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pip-24.3.1           | 1.2 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "conda-24.7.1         | 940 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ncurses-6.5          | 868 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libarchive-3.7.4     | 851 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libsqlite-3.46.0     | 845 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-14.2.0        | 829 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "setuptools-75.6.0    | 756 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libxml2-2.12.7       | 688 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ld_impl_linux-64-2.4 | 654 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "zstd-1.5.6           | 542 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libsolv-0.7.29       | 460 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-14.2.0       | 450 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcurl-8.8.0        | 401 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "zstandard-0.23.0     | 399 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "brotli-python-1.1.0  | 341 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pip-24.3.1           | 1.2 MB    | :   1% 0.013179232412674715/1 [00:00<00:11, 11.21s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "openssl-3.4.0        | 2.8 MB    | :   1% 0.005558673111072358/1 [00:00<00:31, 31.64s/it]\u001b[A\n",
            "\n",
            "libmamba-1.5.8       | 1.6 MB    | :   1% 0.009804167968184325/1 [00:00<00:17, 18.03s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libstdcxx-14.2.0     | 3.7 MB    | :   0% 0.004207828296772089/1 [00:00<00:49, 49.26s/it]\n",
            "libstdcxx-14.2.0     | 3.7 MB    | :  46% 0.4586532843481577/1 [00:00<00:00,  1.87it/s]  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ncurses-6.5          | 868 KB    | :   2% 0.018427913610156946/1 [00:00<00:19, 19.87s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libarchive-3.7.4     | 851 KB    | :   2% 0.01879215876988437/1 [00:00<00:20, 20.52s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "conda-24.7.1         | 940 KB    | :   2% 0.017013799828242037/1 [00:00<00:24, 25.21s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libsqlite-3.46.0     | 845 KB    | :   2% 0.018933467075597506/1 [00:00<00:22, 23.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-14.2.0        | 829 KB    | :   2% 0.019303795604097816/1 [00:00<00:24, 25.27s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "setuptools-75.6.0    | 756 KB    | :   2% 0.021161069005956715/1 [00:00<00:23, 23.75s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ld_impl_linux-64-2.4 | 654 KB    | :   2% 0.024482562300978315/1 [00:00<00:20, 21.46s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libxml2-2.12.7       | 688 KB    | :   2% 0.02324024375021277/1 [00:00<00:22, 23.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "zstd-1.5.6           | 542 KB    | :   3% 0.029528914329381487/1 [00:00<00:19, 20.29s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libsolv-0.7.29       | 460 KB    | :   3% 0.03480980385491383/1 [00:00<00:17, 17.94s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcurl-8.8.0        | 401 KB    | :   4% 0.03994558194646941/1 [00:00<00:15, 15.69s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-14.2.0       | 450 KB    | :   4% 0.0355407469110093/1 [00:00<00:17, 17.85s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "zstandard-0.23.0     | 399 KB    | :   4% 0.04012647284287145/1 [00:00<00:15, 16.15s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "brotli-python-1.1.0  | 341 KB    | :   5% 0.04685587471544436/1 [00:00<00:13, 14.28s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "krb5-1.21.3          | 1.3 MB    | : 100% 1.0/1 [00:00<00:00,  1.37it/s]                 \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "krb5-1.21.3          | 1.3 MB    | : 100% 1.0/1 [00:00<00:00,  1.37it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "libmamba-1.5.8       | 1.6 MB    | : 100% 1.0/1 [00:01<00:00,  1.01s/it]                 \u001b[A\u001b[A\n",
            "\n",
            "libmamba-1.5.8       | 1.6 MB    | : 100% 1.0/1 [00:01<00:00,  1.01s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pip-24.3.1           | 1.2 MB    | : 100% 1.0/1 [00:01<00:00,  1.13s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pip-24.3.1           | 1.2 MB    | : 100% 1.0/1 [00:01<00:00,  1.13s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "openssl-3.4.0        | 2.8 MB    | : 100% 1.0/1 [00:01<00:00,  1.62s/it]               \u001b[A\n",
            "openssl-3.4.0        | 2.8 MB    | : 100% 1.0/1 [00:01<00:00,  1.62s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libarchive-3.7.4     | 851 KB    | : 100% 1.0/1 [00:01<00:00,  1.50s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libstdcxx-14.2.0     | 3.7 MB    | : 100% 1.0/1 [00:01<00:00,  2.09s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libsqlite-3.46.0     | 845 KB    | : 100% 1.0/1 [00:01<00:00,  1.87s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libsqlite-3.46.0     | 845 KB    | : 100% 1.0/1 [00:01<00:00,  1.87s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "conda-24.7.1         | 940 KB    | : 100% 1.0/1 [00:02<00:00,  2.53s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "conda-24.7.1         | 940 KB    | : 100% 1.0/1 [00:02<00:00,  2.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-14.2.0        | 829 KB    | : 100% 1.0/1 [00:02<00:00,  2.65s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-14.2.0        | 829 KB    | : 100% 1.0/1 [00:02<00:00,  2.65s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "setuptools-75.6.0    | 756 KB    | : 100% 1.0/1 [00:03<00:00,  3.23s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "setuptools-75.6.0    | 756 KB    | : 100% 1.0/1 [00:03<00:00,  3.23s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ncurses-6.5          | 868 KB    | : 100% 1.0/1 [00:03<00:00,  3.29s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ncurses-6.5          | 868 KB    | : 100% 1.0/1 [00:03<00:00,  3.29s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ld_impl_linux-64-2.4 | 654 KB    | : 100% 1.0/1 [00:03<00:00,  3.32s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ld_impl_linux-64-2.4 | 654 KB    | : 100% 1.0/1 [00:03<00:00,  3.32s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libxml2-2.12.7       | 688 KB    | : 100% 1.0/1 [00:03<00:00,  3.33s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libxml2-2.12.7       | 688 KB    | : 100% 1.0/1 [00:03<00:00,  3.33s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcurl-8.8.0        | 401 KB    | : 100% 1.0/1 [00:03<00:00,  3.35s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcurl-8.8.0        | 401 KB    | : 100% 1.0/1 [00:03<00:00,  3.35s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "zstd-1.5.6           | 542 KB    | : 100% 1.0/1 [00:03<00:00,  3.37s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "zstd-1.5.6           | 542 KB    | : 100% 1.0/1 [00:03<00:00,  3.37s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libsolv-0.7.29       | 460 KB    | : 100% 1.0/1 [00:03<00:00,  3.45s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libsolv-0.7.29       | 460 KB    | : 100% 1.0/1 [00:03<00:00,  3.45s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-14.2.0       | 450 KB    | : 100% 1.0/1 [00:03<00:00,  3.47s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-14.2.0       | 450 KB    | : 100% 1.0/1 [00:03<00:00,  3.47s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "zstandard-0.23.0     | 399 KB    | : 100% 1.0/1 [00:03<00:00,  3.49s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "zstandard-0.23.0     | 399 KB    | : 100% 1.0/1 [00:03<00:00,  3.49s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "brotli-python-1.1.0  | 341 KB    | : 100% 1.0/1 [00:03<00:00,  3.51s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "brotli-python-1.1.0  | 341 KB    | : 100% 1.0/1 [00:03<00:00,  3.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \n",
            "                                                                        \u001b[A\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "Preparing transaction: - \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Verifying transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr4zH31lF9Fp"
      },
      "source": [
        "Execute the code in the cell below to import the packages required to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda remove pymc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4bNLrx0H8Eh",
        "outputId": "98a3ab21-223c-4470-bf7e-f9ddc24e24de"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PackagesNotFoundError: The following packages are missing from the target environment:\n",
            "  - pymc\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda install cachetools, PYMC"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjPo3PbrIAtA",
        "outputId": "6cf349e0-f4c8-4c07-92af-3895e6675e1e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Channels:\n",
            " - conda-forge\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: \\ \b\b| \b\b/ \b\b- \b\bfailed\n",
            "\n",
            "PackagesNotFoundError: The following packages are not available from current channels:\n",
            "\n",
            "  - cachetools\n",
            "\n",
            "Current channels:\n",
            "\n",
            "  - https://conda.anaconda.org/conda-forge\n",
            "\n",
            "To search for alternate channels that may provide the conda package you're\n",
            "looking for, navigate to\n",
            "\n",
            "    https://anaconda.org\n",
            "\n",
            "and use the search bar at the top of the page.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxV7gVXiF9Fp",
        "outputId": "29cb6ca8-a87d-4b5a-ddbd-6b33a4b507d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.18.2\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numpy.random as nr\n",
        "import scipy.stats as ss\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pymc\n",
        "import arviz as az\n",
        "print(pymc.__version__)\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set(style='ticks', palette='Set2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2iAaF8vF9Fq"
      },
      "source": [
        "### What is a Markov process?\n",
        "\n",
        "As you might guess from the name, a MCMC sampling uses a chain of **Markov sampling processes**. A Markov process is a **stochastic process** that a makes transition from a current state, $X_t$, to some next state, $X_{t+1}$, with some probability $\\Pi$. A Markov process has **no dependency on past states**. We can summarize properties of a Markov process:  \n",
        "- $X_t$ is the **state vector** of **state probabilities** for $N$ possible states  \n",
        "   $$X_t = [x_1, x_2, x_3, \\ldots, x_N]$$\n",
        "- The probability of transition from one state to another is parameterized by a matrix of probabilities, $\\Pi$, of dim N X N for N possible state transitions,  \n",
        "- $\\Pi$  only depends on the current state, $X_t$,     \n",
        "- The transition can be to current state.   \n",
        "\n",
        "Since a Markov transition process depends only on the current state and not the history, we say a Markov process is **memoryless**. We can express the sequence of a Markov transition processes as:\n",
        "\n",
        "$$p(X_{t + 1}| X_t, X_{t-1}, X_{t-2}, \\ldots, X_0) = p(X_{t + 1}| X_t)$$\n",
        "\n",
        "Notice that, since the Markov process is memoryless, the transition probability only depends on the current state, $x_t$. There is no dependency on any previous states, $X_{t-1}, X_{t-2}, \\ldots, X_0$.\n",
        "\n",
        "For a system with $N$ possible states we can write the **transition probability matrix**, $\\Pi$, from one state to another as follows:\n",
        "\n",
        "\\begin{align}\n",
        "\\Pi &=\n",
        "\\begin{bmatrix}\n",
        "\\pi_{1,1} & \\pi_{1,2} & \\cdots & \\pi_{1, N}\\\\\n",
        "\\pi_{2,1} & \\pi_{2,2} & \\cdots & \\pi_{2,N}\\\\\n",
        "\\cdots & \\cdots & \\cdots & \\cdots \\\\\n",
        "\\pi_{N,i} & \\pi_{N,2} & \\cdots & \\pi_{N,N}\n",
        "\\end{bmatrix}\\\\\n",
        "&where\\\\\n",
        "\\pi_{i,j} &= probability\\ of\\ transition\\ state\\ x_j\\ to\\ state\\ x_i\\\\\n",
        "&and\\\\\n",
        "\\pi_{i,i} &= probability\\ of\\ staying\\ in\\ state\\ x_i\\\\\n",
        "&further\\\\\n",
        "\\pi_{i,j} &\\ne \\pi_{j,i}\\ in\\ general\n",
        "\\end{align}\n",
        "\n",
        "Notice that the probability of transition does not depend on the previous state history."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBU-_1fFF9Fq"
      },
      "source": [
        "### Example of a Markov Process\n",
        "\n",
        "To make the foregoing more concrete let's construct a simple example. We will start with a system of 3 states, $\\{ x_1, x_2, x_3 \\}$. The transition matrix is:    \n",
        "\n",
        "$$\\Pi =\n",
        "\\begin{bmatrix}\n",
        "\\pi_{1,1} & \\pi_{1,2} & \\pi_{1,3}\\\\\n",
        "\\pi_{2,1} & \\pi_{2,2} & \\pi_{2,3}\\\\\n",
        "\\pi_{3,1} & \\pi_{3,2} & \\pi_{3,3}\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "0.5 & 0.0 & 0.6\\\\\n",
        "0.2 & 0.3 & 0.4\\\\\n",
        "0.3 & 0.7 & 0.0\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "There are some key points to notice in this transition probability matrix.   \n",
        "- The probabilities of transition from a state is given in each column. Necessarily, the probabilities in each column must add to 1.0.  \n",
        "- The probabilities of a transition to the same state are given along the diagonal of the matrix.   \n",
        "- Some transitions are not possible. These transitions have a probability of 0.0.    \n",
        "\n",
        "Let's apply this probability matrix to a set of three possible states. As an example, let the **state vector** represent being in the first state at time step $t$; $\\vec{X_t} = [1,0,0]$. After a state transition, we compute the probability of being in each of the three possible states at the next time step, $t+1$, as:  \n",
        "\n",
        "$$\\vec{X}_{t+1}  = \\Pi\\ \\vec{X}_t =\n",
        "\\begin{bmatrix}\n",
        "0.5 & 0.0 & 0.6\\\\\n",
        "0.2 & 0.3 & 0.4\\\\\n",
        "0.3 & 0.7 & 0.0\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "1\\\\\n",
        "0\\\\\n",
        "0\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "0.5 \\\\\n",
        "0.2 \\\\\n",
        "0.3\n",
        "\\end{bmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA-CRnDDF9Fq"
      },
      "source": [
        "> **Exercise 29-1:**  Based on the transition probability matrix above, answer the following questions:   \n",
        "> 1. Which state cannot transition to the same state?\n",
        "> 2. What is the minimum number of state transitions required to transition from the second state (second column) to the first state (first column)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABBtTF2cF9Fq"
      },
      "source": [
        "> **Answers:**    \n",
        "> 1.           \n",
        "> 2.                  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pM5Be7VKF9Fq"
      },
      "source": [
        "### From Markov process to Markov chain    \n",
        "\n",
        "So far, we have only discussed a single step Markov process. That is, the process for a single state transition. What happens when there is a series of transitions? A sequence of such transitions is known as a **Markov chain**. There are two major behaviors observed with Markov Chains:  \n",
        "1. **Episodic Markov chains** have a **terminal state**. The terminal state can only transition to itself. Once the system is in the terminal state, we say that the episode has ended. Episodic processes are not of direct interest here, and we will not pursue them further.   \n",
        "2. **Continuous Markov chains** have no terminal state and continue indefinitely, at least in principle. Continuous Markov chains sample probability distribution, and are ideal for estimating Bayesian posterior distributions.  \n",
        "\n",
        "As already indicated, a Markov chain comprises a number of state transitions, one after another. Consider a chain of $n$ state transitions, $\\{t_1, t_2, t_3, \\ldots, t_n \\}$. Each transition in this process has the probabilities given by the state transition matrix, $\\Pi$.            \n",
        "\n",
        "To estimate the probabilities of being in the states we use a special case known as a **stationary Markov chain**. We will not discuss the technical mathematical details here. Here we will just summarize the key result: Over a large number of time steps the number of times the states are visited is proportional to the state probabilities. Starting with some initial state, $\\vec{X}_0$, we can write the relationship as a continuous Markov chain:   \n",
        "\n",
        "$$\\Pi\\ \\Pi\\ \\Pi\\ \\ldots \\Pi\\ \\vec{X}_t = \\Pi^n\\ \\vec{X}_t  \\xrightarrow[\\text{$n \\rightarrow \\infty$}]{} \\vec{p(X)}$$    \n",
        "\n",
        "Notice that in the above, we can find the probabilities of the states without ever actually knowing the values of the transition matrix, $\\Pi$. As long as we can repeatedly sample the stochastic Markov process, we can estimate the state probabilities. This is the key concept of Markov Chain Monte Carlo sampling.\n",
        "\n",
        "As we proceed with this lesson we will use stationary Markov chains to estimate posterior probabilities of Bayesian models. The method relies on the result that the state probabilities converge eventually. As a consequence, we can develop flexible methods for sampling posterior distributions using Markov chain Monte Carlo. In summary, the model is sampled using a Markov chain and the sampled distribution should converge to the posterior distribution.    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTIk4bgaF9Fq"
      },
      "source": [
        "> **Exercise 29-2:** After a large number of transitions, the probabilities of states reach a steady state. In the cell below create and execute code to compute the steady state probabilities using 1000 state transitions. Use the transition probability matrix shown above. Notice that starting state vales do not matter.       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpMVZQPbF9Fq"
      },
      "outputs": [],
      "source": [
        "## Put your code bwlow\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqc6BrkLF9Fq"
      },
      "source": [
        "## MCMC and the Metropolis-Hastings Algorithm\n",
        "\n",
        "Using the principle of Markov chains, a number of MCMC algorithms have been developed over time to sample posterior distributions of Bayesian models. The first MCMC sampling algorithm developed is the **Metropolis-Hastings (M-H) algorithm** (Metropolis et al. (1953), Hastings (1970)). This algorithm is often referred to as simply the Metropolis algorithm or the M-H algorithm.\n",
        "\n",
        "The M-H algorithm has the following steps to estimate the posterior density of the parameters:\n",
        "1. Pick a starting point in the parameter space       \n",
        "2. Choose a nearby point in parameter space randomly from a **sampling distribution**   \n",
        "3. Evaluate the posterior at this point   \n",
        "   - Product of the likelihood $P(data|parameters)$ and prior, $P(parameters)$     \n",
        "4. Use the following **decision rule to accept or reject** the new sample:\n",
        "  - If the likelihood, $p(data | parameters)$, of the new point is greater than your current point, accept new point and move there.\n",
        "  - If the likelihood of the new point is less than your current point, only accept with probability according to the ratio:  \n",
        "$$Acceptance\\ probability\\ = \\frac{p(data | new\\ parameters)}{p(data | previous\\ parameters)}$$.\n",
        "5. If the sample is accepted, compute the posterior density at the new sample point    \n",
        "6. Repeat steps, 2, 3, 4 and 5, many times, until convergence     \n",
        "\n",
        "\n",
        "Eventually, this algorithms converges and the posterior distribution is estimated. The M-H random sampling algorithm is far more **sample efficient** than naive grid sampling. To build some intuition, consider that since the M-H algorithm probabilistically samples the parameter space we only need to visit a limited number of points, rather than sample an entire grid."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxK33ZZGF9Fq"
      },
      "source": [
        "Now that we have outlined the basic Metropolis-Hastings MCMC algorithm, let's examine some of its properties.\n",
        "- The M-H algorithm is **guaranteed to eventually converge** to the underlying distribution. But as a practical issue, convergence can be quite slow. The convergence can be too slow to be useful for complex problems with high-dimensional parameter spaces.   \n",
        "- If there is high **serial correlation** from one sample to the next in M-H chain converges slowly. In this case we say the Markov chain has low **sample efficiency**.\n",
        "- To ensure efficient convergence the algorithm must be ‚Äòtuned‚Äô. The tuning involves finding a good dispersion parameter value for the state sampling distribution. This parameter determines the size of the jumps the algorithm makes in the parameter space. For example if we use Normal distribution we must pick the variance, $\\sigma^2$. If $\\sigma^2$ is too small, the chain will only search the space slowly, using small jumps. If $\\sigma^2$ is too big, there are large jumps which also slows convergence, since the sampling of high density regions will be less likely."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WH79gzeHF9Fq"
      },
      "source": [
        "### M-H algorithm example\n",
        "\n",
        "Let's make these concepts concrete, by trying a simple example. We will find a sample estimate of the probability density of a bivariate Normal distribution.     \n",
        "\n",
        "As a first step, lets plot a set of points with density determined by the bivariate Normal distribution. Execute the code below and examine the resulting plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2qWDiiXF9Fr"
      },
      "outputs": [],
      "source": [
        "def plot_bi_variate(x, title='Draws from a bivariate Normal distribution'):\n",
        "    ## Plot bi-variable points\n",
        "    fig, ax = plt.subplots(figsize=(6,6))\n",
        "    ax.scatter(x[:, 0], x[:, 1], alpha=.2,  marker='o', s=10)\n",
        "    ax.set_xlabel('X')\n",
        "    ax.set_ylabel('Y')\n",
        "    _=ax.set_title(title)\n",
        "\n",
        "np.random.seed(7799)\n",
        "## Define the covariance and mean of the bivariate Normal.\n",
        "sigma = np.array([[1, .6], [.6, 1]])\n",
        "mu = np.array([.5, .5])\n",
        "## Sample 5000 realizations from the bivariate Normal\n",
        "random_points = np.random.multivariate_normal(mean=mu, cov=sigma,  size=5000)\n",
        "\n",
        "## Plot the result\n",
        "plot_bi_variate(random_points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvMsMzfpF9Fr"
      },
      "source": [
        "This plot looks as expected. The density of the dots is proportional to the probability density. You can see the effect of the covariance structure in the elliptical shape of the cloud of points.\n",
        "\n",
        "As a next step, let's look at the density of the marginal distributions of the $X$ and $Y$ variables. The code in the cell below plots histogram and density plots of the marginals. Execute this code and examine the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mR__0en_F9Fr"
      },
      "outputs": [],
      "source": [
        "def plot_marginals(x, bins=40, alpha=0.5):\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(12,3))\n",
        "    ax[0].hist(x[:, 0], density=True, bins=bins, alpha=alpha)\n",
        "    ax[0].set_title('Marginal X distribution')\n",
        "    ax[0].set_xlabel('X')\n",
        "    ax[1].hist(x[:, 1], density=True, bins=bins, alpha=alpha)\n",
        "    ax[1].set_title('Marginal Y distribution')\n",
        "    ax[1].set_xlabel('Y')\n",
        "\n",
        "plot_marginals(random_points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rW47Q-SF9Fr"
      },
      "source": [
        "> **Exercise 29-3:** Examine the plots above and answer the following questions.   \n",
        "> 1. Do these marginal distributions appear approximately Normal?\n",
        "> 2. Do these marginal distributions exhibit any noticeable skewness or heavy tails? Is this behavior to be expected?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvC5gaoGF9Fr"
      },
      "source": [
        "> **Answers:**      \n",
        "> 1.             \n",
        "> 2.               "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ggd8rdmF9Fr"
      },
      "source": [
        "Now, we are ready to sample these data using the M-H MCMC algorithm. The code in the cell below performs the following operations:\n",
        "\n",
        "1. Compute the likelihood of the bi-variate Normal distribution.\n",
        "2. Initialize the chain.\n",
        "3. Initialize some performance statistics.\n",
        "4. Sample the likelihood of the data using the M-H algorithm.\n",
        "5. Plot the result.\n",
        "\n",
        "Execute this code and examine the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETwdV1-SF9Fr"
      },
      "outputs": [],
      "source": [
        "# Calculate the likelihood of a vector `x` for a multivariate normal\n",
        "# distribution MVN(mu, sigma)\n",
        "def likelihood(x, mu, sigma):\n",
        "    return ss.multivariate_normal.pdf(x, mu, sigma)\n",
        "\n",
        "# Initialize the output array\n",
        "chain_length = 2000\n",
        "chain = np.zeros((chain_length,2))\n",
        "# where to start\n",
        "chain[0,:] = [4.0,-4.0]\n",
        "\n",
        "\n",
        "def M_H_sample(start, chain_length, x):\n",
        "    ## Evaluate the current position\n",
        "    current_likelihood = likelihood(start[0], mu, sigma)\n",
        "    # Keep track of how often we accept or reject a proposal\n",
        "    accept_count = 0\n",
        "    reject_count = 0\n",
        "\n",
        "    for i in range(chain_length-1): # chain length minus 1 because we already have a point (the starting point)\n",
        "        # Sample the direction of the move we'll propose\n",
        "        delta = nr.multivariate_normal([0, 0], np.diag([.1, .1]))\n",
        "        # Our new proposal point is our previous position plus the sampled move\n",
        "        proposed = chain[i,:] + delta\n",
        "        proposed_likelihood = likelihood(proposed, mu, sigma)\n",
        "\n",
        "        ## Accept according to probability\n",
        "        ## Two cases, are taken care of by one if statement since the uniform\n",
        "        ## distribution is on the range [0-1], exceeding a random value is the\n",
        "        ## positive decision.\n",
        "        if (nr.uniform() < (proposed_likelihood / current_likelihood)):\n",
        "            accept_count += 1\n",
        "            current_likelihood = proposed_likelihood\n",
        "            chain[i+1,:] = proposed\n",
        "        else:\n",
        "            chain[i+1,:] = chain[i,:]\n",
        "            reject_count += 1\n",
        "    ## Return the result\n",
        "    return accept_count, reject_count, chain\n",
        "\n",
        "np.random.seed(1211)\n",
        "## Sample the data distribution\n",
        "accept_count, reject_count, chain = M_H_sample(chain, chain_length, x=[0,0])\n",
        "print(chain.shape)\n",
        "## Plot the result\n",
        "plot_bi_variate(chain, title='MCMC values for bivariate normal')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcGrE6YyF9Fs"
      },
      "source": [
        "Notice the long 'tail' on the sampled distribution. This behavior arrises from the initial wandering of the Markov chain as it finds the high probability regions of the distribution. This period in which the Markov chain wanders is known as the **burn-in period**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG-NHMw0F9Fs"
      },
      "source": [
        "> **Exercise 29-4:** You will now investigate the properties of the MCMC burn-in period by creating the following plots, with the burn-in period $= 0.5\\ \\times$ chain length:      \n",
        "> 1. Create a scatter plot of the X, Y variables for the burn-in period the Markov chain using the `plot_bi_variate` function.   \n",
        "> 2. Create plots of the marginal distributions of X and Y, using the `plot_marginals()` function.    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqbbZKcgF9Fs"
      },
      "outputs": [],
      "source": [
        "num_burnin = round(.5 * chain_length)\n",
        "## Add your code here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LndO1EfHF9Fs"
      },
      "source": [
        "> Examine you plot and answer these questions.\n",
        "> 1. How can you describe the tail sample and how can you explain the reason for the 'trail' given the initial sample value?  \n",
        "> 2. What evidence do you see that the sampling is converging toward the actual distribution of these data values?   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8puoEE_sF9Fs"
      },
      "source": [
        " > **Answers:**     \n",
        "> 1.          \n",
        "> 2.                  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGtzMEIoF9Fs"
      },
      "source": [
        "> **Exercise 29-5:**  Next, plot the density of the marginal distributions for X and Y of the MCMC samples beyond the burn-in period."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzOzAgipF9Fs"
      },
      "outputs": [],
      "source": [
        "## Put your code below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVbAEbj7F9Ft"
      },
      "source": [
        "> Compare your plots of the MCMC sample marginal distributions with the marginal distribution of the original data samples you examined for Exercise 29-3. Do the MCMC sample marginal distributions look reasonably similar to the marginal distributions of the original data samples? If not, what differences do you see?      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq6zjo3kF9Ft"
      },
      "source": [
        "> **Answer:**                "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnPJAhQuF9Ft"
      },
      "source": [
        "> **Exercise 29-6:** Next, you will compare the **Maximum a posteriori or MAP** point of the sampled marginal distributions to the original means for $X=0.5$ and $Y=0.5$. Using the [Numpy mean function](https://numpy.org/doc/stable/reference/generated/numpy.mean.html) and [numpy.linalg.norm](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html) compute and display an approximation of the MAP  and the Euclidean norm of the error for the MCMC sampled distributions:   \n",
        "> 1. All samples.      \n",
        "> 2. Samples in the burn-in period.     \n",
        "> 3. Samples after the burn-in period.     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OHSwhhPF9Ft"
      },
      "outputs": [],
      "source": [
        "answer = np.array([0.5,0.5])\n",
        "mcmc_map_all = np.mean(chain, axis=0)\n",
        "#https://numpy.org/doc/stable/reference/generated/numpy.amax.html\n",
        "error_all = np.linalg.norm(mcmc_map_all - answer)\n",
        "print('MAP for all samples = ' + str(mcmc_map_all) + ', with error norm = ' + str(error_all))\n",
        "mcmc_map_burnin = np.mean(chain[:num_burnin,:], axis=0)\n",
        "error_burnin = np.linalg.norm(mcmc_map_burnin - answer)\n",
        "print('MAP for burn-in period = ' + str(mcmc_map_burnin) + ', with error norm = ' + str(error_burnin))\n",
        "mcmc_map = np.mean(chain[num_burnin:,:], axis=0)\n",
        "error_bulk = np.linalg.norm(mcmc_map - answer)\n",
        "print('MAP after burn-in period = ' + str(mcmc_map) + ', with error norm = ' + str(error_bulk))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I69_B2mF9Ft"
      },
      "source": [
        "> Compare the results of your MAP estimates to the original data with $X = 0.5$ and $Y = 0.5$ and answer these questions.\n",
        "> 1. Does the MAP estimate, excluding the burn-in period, appear to be a reasonable estimate for the original data sample?\n",
        "> 2. Why does excluding the burn-in period from the MAP estimate improve it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuSfTgkuF9Ft"
      },
      "source": [
        "> **Answers:**       \n",
        "> 1.           \n",
        "> 2.               "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFUV_x7kF9Ft"
      },
      "source": [
        "### Convergence and sampling efficiency of MCMC\n",
        "\n",
        "Let's turn our attention to the convergence properties of the M-H MCMC sampler. While convergence of MCMC sampling to the underlying distribution generally occurs, it can be slow. Unfortunately, it is not unusual for convergence to be too slow to be of practical use. Further, in some pathological cases, convergence may not occur at all.  \n",
        "\n",
        "The **acceptance rate** and **rejection rate** are key convergence statistics for the M-H algorithm. A low acceptance rate and high rejection rate are signs of poor convergence. Likewise, too few rejections, indicate that the algorithm is not exploring the parameter space sufficiently. The trade-off between these statistics is controlled by the dispersion of the sampling distribution. This hyperparameter is generally determined by trial and error. Unfortunately, there are few useful rules of thumb one can use.     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KjRD5jZF9Ft"
      },
      "outputs": [],
      "source": [
        "print('Acceptance rate = %.2f' % (accept_count / chain_length))\n",
        "print('Rejection rate = %.2f' % (reject_count / chain_length))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq_tuWhkF9Ft"
      },
      "source": [
        "These statistics indicate good convergence with a reasonable rejection rate.\n",
        "\n",
        "Another way to evaluate the convergence of MCMC algorithms is to look at the **trace** of the samples. The trace is a plot of the sample value with sample number. The code in the cell below plots the trace for both the $x$ and $y$ samples, including the burn-in period. Execute this code and examine the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meRApoJ1F9Fu"
      },
      "outputs": [],
      "source": [
        "def plot_traces(x):\n",
        "    fig, ax = plt.subplots(2, 1, figsize=(12,6))\n",
        "    ax[0].plot(x[:, 0])\n",
        "    ax[0].set_title('X chain')\n",
        "    ax[0].set_ylabel('Value')\n",
        "    ax[1].plot(x[:, 1])\n",
        "    ax[1].set_title('Y chain')\n",
        "    ax[1].set_xlabel('Sample number')\n",
        "    ax[1].set_ylabel('Value')\n",
        "\n",
        "plot_traces(chain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io0xDanaF9Fu"
      },
      "source": [
        "Examine these sample traces. The nature of these plots gives insight into the progression of the MCMC sampling. Notice that there is a significant excursion during the initial burn-in period. After the initial burn-in you can see that the sampling wanders around the mode of the distribution, as it should."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4X4nIwhnF9Fu"
      },
      "source": [
        "> **Exercise 29-7:** You will now look at a close-up view of the traces just after the burn-in period. Create the code in the cell below to make trace plots of samples beyond the burn-in period. Execute your code and examine the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boyS4FF4F9Fu"
      },
      "outputs": [],
      "source": [
        "## Put your code below\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nkt9bY_EF9Fu"
      },
      "source": [
        "> Examine your plots and answer the following questions:   \n",
        "> 1. Do the samples appear to be centered around the MAP in the densest part of the joint distribution, $P(X,Y)$?\n",
        "> 2. Do you consider the foregoing behavior ideal in terms of how the M-H algorithm samples and, and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khAl0SJVF9Fu"
      },
      "source": [
        "> **Answers:**  \n",
        "> 1.              \n",
        "> 2.                "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6OK3Lx1F9Fu"
      },
      "source": [
        "Finally, let's take a look at the autocorrelation of our MCMC samples. Execute the code in the cell below, which uses the [Pandas.plotting.autocorrelation_plot](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.plotting.autocorrelation_plot.html) function to display the autocorrelation of each chain.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kt6uqK5IF9Fu"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(12,4))\n",
        "ax[0].set_title('Autocorrelation of X');\n",
        "pd.plotting.autocorrelation_plot(chain[1000:2000, 0], ax=ax[0]);\n",
        "ax[1].set_title('Autocorrelation of Y');\n",
        "pd.plotting.autocorrelation_plot(chain[1000:2000, 1], ax=ax[1]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47h5FNnTF9Fu"
      },
      "source": [
        "Notice that the autocorrelation dies off fairly quickly with lag. We can relate sampling efficiency to the autocorrelation of the samples. Intuitively, uncorrelated samples provide maximum information on the distribution being sampled. But, if there is significant autocorrelation, the new information gathered per-sample will be reduced, perhaps greatly so.   \n",
        "\n",
        "We can compute an **effective sample size or ESS**. ESS is the ratio between the number of samples adjusted for the autocorrelation and the hypothetical number of uncorrelated samples. In other words, the ratio of actual vs. ideal sampling. For a sample of size $N$, and autocorrelation function at lag k$, $ACF(k)$,we compute the ESS as follows:\n",
        "\n",
        "$$ESS = \\frac{N}{1 + 2 \\sum_k ACF(k)}$$    \n",
        "\n",
        "If the autocorrelation is low, the number of effective samples is high. However, if there is significant autocorrelation the ESS will be significantly less than the actual number of samples.\n",
        "\n",
        "We will return to this concept with an example latter in this lesson."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3_Ht0e0F9Fu"
      },
      "source": [
        "## Other MCMC Sampling Algorithms\n",
        "\n",
        "Now that you have some experience with the Metropolis-Hastings MCMC algorithm, let's examine some other MCMC sampling methods. The Metropolis-Hastings algorithm is a useful tool. However, this algorithm can suffer from slow convergence for several reasons:\n",
        "\n",
        "- Samples from the M-H algorithm generally have a fairly high serial correlation, resulting in is low ESS.\n",
        "- As already discussed, one must ‚Äòtune‚Äô the state selection probability distribution. For example, if we use a Normal sampling distribution we must pick $\\sigma$. If $\\sigma$ is too small, the chain will only search the space slowly, with small jumps. If $\\sigma$ is too big, the the jumps are too large, slowing convergence.\n",
        "\n",
        "As a result of these limimitations, quite a number of MCMC sampling methods have been proposed in a quest to improve sample efficiency. Here, we will only address a few widely used choices.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6APt80Y7F9Fv"
      },
      "source": [
        "### Gibbs sampling\n",
        "\n",
        "The Gibbs sampler (Geman and Geman, 1984) is an improved MCMC sampler which speeds convergence. The Gibbs sampler is named for the 19th Century physicist Josiah Willard Gibbs and is inspired by statistical mechanics.   \n",
        "\n",
        "In contrast to the M-H algorithm, the Gibbs sampler samples each dimension of the parameter space sequentially in a round-robin manner. Whereas, the M-H algorithm attempts jumps across all dimensions of the parameter space.\n",
        "\n",
        "The basic Gibbs sampler algorithm has the following steps:\n",
        "\n",
        "1. For an N dimensional parameter space, $\\{ \\theta_1, \\theta_2, \\ldots, \\theta_N \\}$, find a random starting point.\n",
        "2. In order, $\\{1, 2, 3, \\ldots, N\\}$, assign the next dimension to sample, starting with dimension $1$.  \n",
        "3. Sample the marginal distribution of the parameter given the observations, $D$, and other parameter values: $p(\\theta_1|D, \\theta_2, \\theta_3, \\ldots, \\theta_N)$.\n",
        "3. Repeat steps 2 and 3 until convergence.    \n",
        "\n",
        "From this simplified description of the Gibbs sampling algorithm you can infer:\n",
        "\n",
        "- When compared to the Metropolis-Hastings algorithm, the Gibbs sampler reduces serial correlation through round-robin sampling. The update along each dimension approximately orthogonal to the preceding sample dimensions.    \n",
        "- There are no tuning parameters since sampling is based on the marginals of the likelihood."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrcctMIoF9Fv"
      },
      "source": [
        "### No U-Turn Sampler   \n",
        "\n",
        "The PyMC3 package uses the No U-Turn Sampler (NUTS) MCMC algorithm. NUTS uses an alternative to proposing new samples with the Metropolis-Hastings acceptance criteria or exploring dimensions in a round-robin fashion as in done for the Gibbs sampler. Instead, NUTS models the exploration as the movement of a particle through a field. In 2-dimensions, this field can be imagined to look like a hilly landscape. The high spots on the hills are the high density regions we want to sample the most. The field that guides the movement of the particle through the space is derived from the target probability distribution, such that the particle is drawn towards dense (high likelihood) regions of the space. This strategy directs the exploration of the space using the gradient, rather than using a random wandering behavior as we saw with the M-H MCMC algorithm.\n",
        "\n",
        "The NUTS sampler is based on an earlier idea, Hamiltonian Monte Carlo. Imagine a ball in a 2-dimensional hilly landscape. The ball follows the gradient, which might send it part of the way up one of the hills. These hills represent the highest density regions of a distribution which we wish to sample. The difficulty with this idea is that one must set a number of time steps to simulate the motion of the ball. If the number of steps is too small, the algorithm will slow exploration of the parameter space. If the number of time steps is too large, the ball will jump around randomly. The original HMC method requires 2 hyperparameters to control stopping criteria of the ball. Unfortunately, in practice, setting these hyperparameters proved tricky, which limited the usefulness of HMC in practice.  \n",
        "\n",
        "The NUTS sampler adds a simple heuristic to the HMC algorithm. The algorithm runs time both forward and backward. The point at which the forward and backward solutions meet, is the U-turn point. The number of time steps is set by finding this equilibrium point. The need for complex stopping criteria, and associated hyperparameters, is eliminated. As a result, NUTS is both a highly efficient sampler and easy to use.\n",
        "\n",
        "Why even discuss other samplers when we have the NUTS algorithm. Unfortunately, while NUTS works well in many common cases, it is not guaranteed to converge. For density functions with highly complex behavior, other samplers are required. In these cases, the Gibbs sampler is typically used.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwOgp2qSF9Fv"
      },
      "source": [
        "## An example: Linear Regression   \n",
        "\n",
        "With the foregoing in mind, let's try a simple example. In this case we will try a simple regression model using synthetic data. The data have two independent (predictor) variables and one response variable. Normally distributed noise is added to the linear trend of the response variable. This example has been derived in part from the [Introductory Overview of PyMC](https://www.pymc.io/projects/docs/en/latest/learn/core_notebooks/pymc_overview.html)  \n",
        "\n",
        "Our model has 4 parameters we wish to estimate and perform inference on. We state our prior distributions for these parameters:    \n",
        "\n",
        "\\begin{align}   \n",
        "\\beta_0 &\\sim \\mathtt{N}(0, 2)\\\\\n",
        "\\beta_1 &\\sim \\mathtt{N}(0, 2)\\\\\n",
        "\\beta_2 &\\sim \\mathtt{N}(0, 2)\\\\\n",
        "\\sigma &\\sim |\\mathtt{N}(0, 1)\n",
        "\\end{align}\n",
        "\n",
        "Here, $\\beta_0$ is the intercept term, $\\beta_1, \\beta_2$ are the partial slopes, $\\sigma$ is the standard deviation. The notation, $|\\mathtt{N}(.,.)$, indicates a **half Normal distribution**, with only values $> 0$.      \n",
        "\n",
        "Given the two independent variables, $x_1, x_2$, and a realization of the values of $[\\beta_0, \\beta_1, \\beta_2]$, the expected value of the response, $\\mu$ is **computed deterministically** as:   \n",
        "\n",
        "$$\\mu = \\beta_0 + \\beta_1 * x_1 + \\beta_2 * x_2$$\n",
        "\n",
        "Finally, the values of the posterior distribution of the response variable, $Y$ are computed as:    \n",
        "\n",
        "$$Y \\sim \\mathtt{N}(\\mu, \\sigma)$$\n",
        "\n",
        "As a first step, execute the code in the cells below to configure the environment and to generate and display the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xh6Yt7Y0F9Fv"
      },
      "outputs": [],
      "source": [
        "az.style.use(\"arviz-darkgrid\")\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "# Initialize random number generation\n",
        "SEED = 8927"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JdM8PhAF9Fv"
      },
      "outputs": [],
      "source": [
        "# True parameter values\n",
        "Betas = [1.0, 1.0, 2.5]\n",
        "sigma = 1.0\n",
        "\n",
        "# Size of sample\n",
        "size = 100\n",
        "\n",
        "# Independent variables\n",
        "np.random.seed(SEED)\n",
        "x1 = np.random.randn(size)\n",
        "x2 = np.random.randn(size) * 0.2\n",
        "\n",
        "# Simulate dependent variable\n",
        "Y = Betas[0] + Betas[1] * x1 + Betas[2] * x2 + np.random.normal(size=size) * sigma\n",
        "\n",
        "print('Mean of response variable Y = {0:4.2f}'.format(np.mean(Y)))\n",
        "print('Means of independent variables, x1 = {0:4.2f}  x2={1:4.2f}'.format(np.mean(x1),np.mean(x2)))\n",
        "\n",
        "## Plot the simulated data\n",
        "fig, axes = plt.subplots(1, 2, sharex=True, figsize=(10, 4))\n",
        "axes[0].scatter(x1, Y, s=8, alpha=0.6);\n",
        "axes[1].scatter(x2, Y, s=8, alpha=0.6);\n",
        "axes[0].set_ylabel(\"Y\");\n",
        "axes[0].set_xlabel(\"x1\");\n",
        "axes[1].set_xlabel(\"x2\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUtHI7h8F9Fv"
      },
      "source": [
        "Examination of these plots show that noting the following:          \n",
        "1. The independent have been defined so that they are **zero centered**.   \n",
        "2. This is difficult problem. There relationship between $x_1$ and $Y$ shows significant dispersion. Further, it appears that $x_2$ has little if any predictive power.     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiM274PdF9Fv"
      },
      "source": [
        "### Defining the model      \n",
        "\n",
        "The code in the cell below defines the PyMC model which has the following steps:       \n",
        "1. The priors are defined. Notice the `shape=3` argument which defines a 3-vector of the betas.    \n",
        "2. The deterministic formulation of the expected value of the response variable is defined.   \n",
        "3. The distribution of the response variable is defined.   \n",
        "\n",
        "Execute the code to instantiate the model.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqh7mFkKF9Fv"
      },
      "outputs": [],
      "source": [
        "with pymc.Model() as regression_model:\n",
        "    # Priors for unknown model parameters\n",
        "    betas = pymc.Normal(\"betas\", mu=0, sigma=2, shape=3)\n",
        "    sigma = pymc.HalfNormal(\"sigma\", sigma=1)\n",
        "\n",
        "    # Deterministic expected value of outcome\n",
        "    mu = betas[0] + betas[1] * x1 + betas[2] * x2\n",
        "\n",
        "    # Likelihood (sampling distribution) of observations\n",
        "    Y_obs = pymc.Normal(\"Y_obs\", mu=mu, sigma=sigma, observed=Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ9yViGrF9Fv"
      },
      "source": [
        "###   Prior predictive checks\n",
        "\n",
        "Before we get started with MCMC sampling and inference, we must verify that we are satisfied with priors.   \n",
        "\n",
        "To start, execute the code in the cell below to compute prior check samples.    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_ooHJLoF9Fw"
      },
      "outputs": [],
      "source": [
        "SEED = 3434\n",
        "with regression_model:\n",
        "    prior_checks = pymc.sample_prior_predictive(samples=50, random_seed=SEED)\n",
        "prior_checks.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyn0sXIrF9Fw"
      },
      "source": [
        "There are several variables we will investigate. First we should compare the prior of the response density to the actual observed density. To do so, execute the code in the cell below and examine the results. The complex reference to the prior density arrises from PyMCs use of the [ArvidZ inference data structures](https://python.arviz.org/en/latest/schema/schema.html).   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzWvpMuIF9Fw"
      },
      "outputs": [],
      "source": [
        "print(sns.__version__)\n",
        "print(pymc.__version__)\n",
        "print(az.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFbwYIk5F9Fw"
      },
      "outputs": [],
      "source": [
        "_,ax=plt.subplots(1,2, figsize=(12,3))\n",
        "ax = ax.flatten()\n",
        "sns.kdeplot(prior_checks['prior_predictive']['Y_obs_dim_0'], ax=ax[0]);\n",
        "#sns.kdeplot(prior_checks['prior_predictive'].to_array()['Y_obs_dim_0'], ax=ax[0]);\n",
        "#sns.kdeplot(prior_checks['prior'].to_array()[1,0,:,1], ax=ax[0]);\n",
        "ax[0].set_title('Prior density of response');\n",
        "sns.kdeplot(Y, ax=ax[1]);\n",
        "ax[1].set_title('Density of observed reponse');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fMlowkTF9Fw"
      },
      "source": [
        "Comparing these densities one can see that the dispersion of the prior density is greater than the actual response. This is not unexpected or undesirable. If the prior was less dispersed it would restrict the posterior. This observation indicates that the prior is not very informative. Overall, we are satisfied with this prior density.     \n",
        "\n",
        "Next, display and then examine density plots of the priors for the model parameters by executing the code in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrJ6V02zF9Fw"
      },
      "outputs": [],
      "source": [
        "az.plot_density(prior_checks, group='prior')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWIB7RRWF9Fw"
      },
      "source": [
        "These densities look reasonable, with no apparent undesirable behavior, but with wide (high) dispersion. Again, wide dispersion of the priors desirable, so as not to overly restrict the posterior. The negative values of the density of $\\sigma$ are an artifact of the kernel density estimation.     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKRZiuYkF9Fw"
      },
      "source": [
        "### MCMC sampling and inference       \n",
        "\n",
        "Now that we are satisfied with the priors, we are ready to MCMC sample the model. Execute the code in the cell below to do so.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YU6bfkKcF9Fw"
      },
      "outputs": [],
      "source": [
        "with regression_model:\n",
        "    np.random.seed(1414)\n",
        "    # 1000 posterior sample draws\n",
        "    idata = pymc.sample(return_inferencedata=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSGxTEUuF9Fw"
      },
      "source": [
        "Now that the traces are computed, execute the code in the cell below to display the densities and the trace plots.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zh9QyZjPF9Fw"
      },
      "outputs": [],
      "source": [
        "az.plot_trace(idata, combined=False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwHiIpMvF9Fx"
      },
      "source": [
        "Examine the plots and notice the following:     \n",
        "1, The posterior density of the betas looks reasonable.    \n",
        "2, The multiple traces for each of the betas overlap and have nearly constant dispersion. This indicates good convergence of the MCMC sampling.    \n",
        "3. The density and traces of $\\sigma$ have similar desirable properties as the betas.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmiHd3fpF9Fx"
      },
      "source": [
        "The forest plot displays the HDI computed from each trace for each of the model parameters. Execute the code in the cell below to display this plot and examine the results.     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFdcyHTQF9Fx"
      },
      "outputs": [],
      "source": [
        "az.plot_forest(idata, var_names=[\"betas\", \"sigma\"]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kGM9ldaF9Fx"
      },
      "source": [
        "Notice that the HDI is very similar for all traces for each of these model parameters. This fact further indicates good convergence of the MCMC sampling. The values appear reasonable as well.              "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Tu3dtARF9Fx"
      },
      "source": [
        "Execute the code in the cell below to display the density and HDI for the model parameters integrated over the MCMC traces.    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "aGxWAAwcF9Fx"
      },
      "outputs": [],
      "source": [
        "az.plot_posterior(idata, var_names=[\"betas\", \"sigma\"]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KTpPm5pF9Fx"
      },
      "source": [
        "One can see that these posterior distributions are approximately Normal, but with some skewness and heavy tails. The HDIs confirm that two $\\beta$s and $\\sigma$ are well determined and third $\\beta$ is not. The HDI of $\\beta_2$ has a range from about 1.3 to about 3.2. This means that a small change in $x_2$ can lead to a large change in the response with high uncertainty.            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI29MtwJF9Fx"
      },
      "source": [
        "### Sampling     \n",
        "\n",
        "The posterior distribution of the parameters seems promising. We now need to perform analysis of the sampling and some posterior checks. We will start with analysis of the sampling.   \n",
        "\n",
        "A first step to check the sampling execute the code in the cell below to print a summary table of some sampling statistics.      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQC2TgjwF9Fx"
      },
      "outputs": [],
      "source": [
        "az.summary(idata, kind='diagnostics')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yrxoUccF9Fx"
      },
      "source": [
        "The summary shows a lot of useful information for each parameter estimate:      \n",
        "1. The mean MCMC error of the posterior distribution of each coefficient value. This estimated error tells us how much error the MCMC sampling has introduced.        \n",
        "2. The standard deviation (`sd`) of the mean error of the posterior distribution of the coefficient values.\n",
        "3. Next come metrics of **effective sample size (ESS)**. First the middle portion of the posterior distribution and then the tail ESS. We have already discussed the concept of effective sample size. These metrics give indications of overall ESS alone with specific metrics for the bunk of the posterior distribution and its tails. These statistics are reported with respect to the total number of samples in all chains.    \n",
        "6. The **Gelman-Rudin statistic** (`R_hat`) (Gelman and Rubin, 1992) measures the ratio of the **variance shrinkage between chains** to the **variance shrinkage within chains**. The Gelman-Rudin statistic should converge to 1.0. That is, if all chains converge, the reduction in variance between chains and within the chains should be the same.   \n",
        "\n",
        "Keeping in mind that there are 1000 samples per trace, you can see that ESS is generally good for these parameters. Further, the standard error and `r_hat` are quite good.        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLKiIhE6F9Fx"
      },
      "source": [
        "To further investigate the MCMC error and the ESS we can create plots by quantiles of the posterior of the parameters. Execute the code in the cell below and examine the results.      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nOl6o9MF9Fy"
      },
      "outputs": [],
      "source": [
        "_,ax = plt.subplots(3,4, figsize=(16,12))\n",
        "az.plot_mcse(idata, ax=ax[0])\n",
        "az.plot_ess(idata, kind=\"local\", ax=ax[1]);\n",
        "az.plot_ess(idata, kind=\"quantile\", ax=ax[2]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiajjXTvF9Fy"
      },
      "source": [
        "Notice the following about these plots:     \n",
        "1. The MCSE is generally small across all quantiles of all parameters. However, MCSE increases at the extreme quantiles, likely as a result of lower sampling density in the tails of the distribution.    \n",
        "2. The local ESS is large compared to the total number of samples in the chains and fairly constant over the quantiles.        \n",
        "3. The quantile ESS is lower at the outer quantiles. Again, this may be a result of limited sample in the tails and dense sampling in the middle part of the distribution.      \n",
        "\n",
        "Overall, these results look satisfactory.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olVpw0WaF9Fy"
      },
      "source": [
        "### Posterior checks   \n",
        "\n",
        "To complete this analysis we must perform the posterior checks. The goal of these checks is to verity that the posterior distribution of the response matches the observations reasonably well. Posterior predictive checks use statistics based on the on **posterior predictive distribution**. Since we do not know the true posterior distribution we approximate the posterior predictive distribution using by resampling from the posterior distribution:    \n",
        "\n",
        "$$p(y^{rep}|y)= \\sum_i p(y^{rep}|\\theta) p(\\theta|y)$$\n",
        "\n",
        "Where:   \n",
        "$y^{rep} =$ realization drawn from the posterior distribution  \n",
        "$y =$ observation  \n",
        "$p(y^{rep}|\\theta) =$ draw from posterior distribution with parameters $\\theta$    \n",
        "$p(\\theta|y) =$ posterior distribution of model parameters, $\\theta$    \n",
        "\n",
        "\n",
        "Now, execute the code in the cell below to sample the response variable from the posterior distribution.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OndJTHO2F9Fy"
      },
      "outputs": [],
      "source": [
        "SEED = 6545\n",
        "with regression_model:\n",
        "    pymc.sample_posterior_predictive(idata, extend_inferencedata=True, random_seed=SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs49oYnYF9Fy"
      },
      "source": [
        "With the posterior predictive samples computed, we can now start the posterior predictive checks.   \n",
        "\n",
        "The first step in the posterior checks is to display the posterior density of the response along with the density of the observed response. Execute the code in the cell below to display the plot.     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WJjwTq-F9Fy"
      },
      "outputs": [],
      "source": [
        "az.plot_ppc(idata, num_pp_samples=100);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byjJ0EGkF9Fy"
      },
      "source": [
        "Examine this plot. Notice the following:    \n",
        "1. The mean posterior predictive and the observed curves are fairly similar. The observed density seems to have more weight in the tails and a bit of right skew, meaning that the mean posterior predictive has a lower probability of forecasting these values.    \n",
        "2. There is considerable dispersion for the posterior predictive distribution.     \n",
        "\n",
        "Next, we will evaluate the **Bayesian p-value** test statistic, $T$, for the distribution differences between observed and predicted responses. This statistic is formulated:  \n",
        "\n",
        "$$p = p(y^{rep} < y | y )$$\n",
        "  \n",
        "Intuitively, the posterior predictive Bayesian p-value should be symmetric and centered on $p = 0.5$, over many draws of $y^{rep}$. In other words, we expect the following behavior:      \n",
        "- $y^{rep} < y | y$ half the time    \n",
        "- $y^{rep} > y | y$ half the time    \n",
        "   \n",
        "To display the density of the Bayesian p-value, execute the code in the cell below.     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTodMtiyF9Fy"
      },
      "outputs": [],
      "source": [
        "_,ax = plt.subplots(figsize=(8,3))\n",
        "az.plot_bpv(idata, kind='p_value', ax=ax);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IogmUTidF9Fy"
      },
      "source": [
        "Examine the plot. The dotted line is the ideal density, centered at $p =0.5$. The empirical density of the Bayesian p-value is shown by the blue line. The blue line is very nearly the same as the dotted line indicating that the density of $p$ is close to the desired behavior of posterior predictive values.   \n",
        "\n",
        "Another statistic is the posterior predictive **Bayesian u-value**, also known as the **marginal p-value**. This statistic can is defined:        \n",
        "\n",
        "$$p_i = p(y_i^{rep} < y_i | y )$$\n",
        "\n",
        "The U-value computed for specific (ordered) observation $y_i$, with the following properties:       \n",
        "- $p_i$ should be centered on 1.0    \n",
        "- $p_i$ should be close to uniformly distributed    \n",
        "\n",
        "Execute the code in the cell below to display the Bayesian u-value.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iY8HGJfMF9Fy"
      },
      "outputs": [],
      "source": [
        "_,ax = plt.subplots(figsize=(8,3))\n",
        "az.plot_bpv(idata, kind='u_value', ax=ax);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO1-CgJiF9Fy"
      },
      "source": [
        "Examine this plot. The white line shows the ideal behavior and the shaded area is the confidence interval. The blue line is very close to the ideal. Again, this indicates the posterior predictive values have a distribution close to the actual values.       "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob4a26s8F9Fz"
      },
      "source": [
        "## Example: British Coal Mine Disasters   \n",
        "\n",
        "Toward the end of the 19th Century there was growing public awareness of the rising accidental death rate in European coal mines. A mine disaster became defined as an incident leading to 6 or more deaths. As early as 1886 a British Royal Commission published recommendations for safety practices. However, adoption of these practices remained voluntary. The [British Coal Mines Act of 1911](https://en.wikipedia.org/wiki/Coal_Mines_Act_1911) finally codified mine safety procedures into law.  \n",
        "\n",
        "Famous data set originally published by Carlin, Gelfand, and Smith (1992) includes the number of coal mine disasters by year between 1851 and 1962. These data are available for download from [Kaggle](https://www.kaggle.com/datasets/nabamitachakraborty/coal-mine-disastersuk), and other sources.  \n",
        "\n",
        "Our goal here is to create a model to quantify the differences in the rate of mine disasters before and after the introduction of improved safety practices. This analysis is an example of a **point process model**. A point process model has an **intensity** or **rate** of occurrence of a type of event, mine disasters in this case. The number events over a time period for this type of point process is independent and identically distributed (iid). In other words, the number of events per time period are a iid draw from some probability distribution. Because of the iid nature of the point process, there is no autoregressive (AR) effect in the time series.       \n",
        "\n",
        "The iid nature of the point process does not mean that the distribution of the process cannot change in time. It fact, such **change points** or **switch points** in rate with time are quite common in the real-world. Identifying these change points is often a challenging problem.   \n",
        "\n",
        "The dataset gives us the rate of mine disasters by year so the problem can be addressed as a point process model. There are two related questions we can ask about these data. First, at what point in time was there a significant change in the rate or intensity of mine disasters, the switch point? The answer is not obvious from the history since the changes in safety practice were adopted over about 25 years. The second question is how significant was the reduction in the rate of disasters over this period?      \n",
        "\n",
        "As is common for point process models, we can try modeling the intensity of the point process using a Poisson distribution. If there is a switch point in time $s$, we can write the rate model for intensity of disasters at time t, $D_t$, as:\"     \n",
        "\n",
        "$$\n",
        "D_t \\sim Pois(r_t),\\ r_t\n",
        "\\begin{cases}\n",
        "     e,\\ t < s\\\\\n",
        "     l,\\ t \\ge s\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "There are now three parameters we must estimate for this model, $e, l$ and $s$. To create a Bayesian model for this problem we need to come up with some prior distributions for these parameters. Let's start with some simple priors:    \n",
        "\n",
        "\\begin{align}   \n",
        "s &\\sim unif(t_{min}, t_{max})\\\\\n",
        "e &\\sim exp(2) \\\\    \n",
        "l &\\sim exp(2)\n",
        "\\end{align}   \n",
        "\n",
        "In words, our prior for the switch point is uniform over all years in the data, and exponential distributions with mean 2 for the rates. We now have everything we need to create and test our model.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VZd6ctUF9Fz"
      },
      "source": [
        "### Load and explore the data\n",
        "\n",
        "To get started execute the code in the cell below to load the data and display a sample.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqlZNC8OF9Fz"
      },
      "outputs": [],
      "source": [
        "disaster_data = pd.read_csv('../data/CoalMiningDisastersUK.csv')\n",
        "disaster_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48obDuJhF9Fz"
      },
      "source": [
        "There are two columns in this table, the year and the rate of mine disasters.   \n",
        "\n",
        "Next, execute the code in the cell below to display a chart of the number of mine disasters vs. time and the distribution of occurrences.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmoT5vmSF9Fz"
      },
      "outputs": [],
      "source": [
        "sns.scatterplot(x='Year', y='Count', data=disaster_data);\n",
        "plt.show()\n",
        "sns.kdeplot(x='Count', data=disaster_data);\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3NFv1JNF9Fz"
      },
      "source": [
        "By examining the plot above, it does appear that there is a change in the rate of mine disasters somewhere between 1880 and 1900. Further, one can see from both charts that many years has no disasters, where some years has as many as 6.   \n",
        "\n",
        "To get a further feel for these data execute the code in the cell below to compute and display the mean intensity rate over the entire time period.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUIk_1PYF9Fz"
      },
      "outputs": [],
      "source": [
        "print(\"Mean intensity rate = {0:4.2f}\".format(np.mean(disaster_data.loc[:,'Count'])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdEnG-LfF9Fz"
      },
      "source": [
        "### Define the Bayesian model   \n",
        "\n",
        "We are now ready to define our model in code as shown below. This code does the following:    \n",
        "1. The prior for the switch point, $s$ uses the pymc [DiscreteUniform]9https://docs.pymc.io/en/v4.3.0/api/distributions/generated/pymc.DiscreteUniform.html) distribution function. Since the counts are aggregated annually, a discrete distribution is appropriate for this prior.     \n",
        "2. The priors for the rates use the pymc [Exponential](https://docs.pymc.io/en/v4.3.0/api/distributions/generated/pymc.Exponential.html) distribution function with parameter 2.0.    \n",
        "3. The switched rate is selected deterministically using the [pymc.math.switch](https://docs.pymc.io/en/latest/api/generated/pymc.math.switch.html) function, based on the switchpoint value.      \n",
        "4. The posterior distribution of the disaster rate is computed using the pymc [Poisson](https://docs.pymc.io/en/latest/api/distributions/generated/pymc.Poisson.html) distribution function using the value of the rate and the observed values, or evidence.    \n",
        "\n",
        "Execute this code to compile the model.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORSRpmUiF9Fz"
      },
      "outputs": [],
      "source": [
        "with pymc.Model() as disaster_model:\n",
        "    # Uniform prior on the switch point\n",
        "    switchpoint = pymc.DiscreteUniform(\"switchpoint\", lower=disaster_data.Year.min(), upper=disaster_data.Year.max())\n",
        "    ## More informative possible priors\n",
        "#    switchpoint = pymc.Triangular(\"switchpoint\", lower=disaster_data.Year.min(), upper=disaster_data.Year.max(), c=1900)\n",
        "#    switchpoint = pymc.Laplace(\"switchpoint\", mu=1890, b=10.0)\n",
        "\n",
        "    # Priors for pre- and post-switch rates\n",
        "    early_rate = pymc.Exponential(\"early_rate\", 2.0)\n",
        "    late_rate = pymc.Exponential(\"late_rate\", 2.0)\n",
        "\n",
        "    # Poisson rate switch for years before and after current\n",
        "    rate = pymc.math.switch(switchpoint >= disaster_data.Year, early_rate, late_rate)\n",
        "\n",
        "    disasters = pymc.Poisson(\"disasters\", rate, observed=disaster_data.Count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STArRO4EF9Fz"
      },
      "source": [
        "### Prior predictive checks   \n",
        "\n",
        "Before proceeding we must verify that we are happy with the prior distribution, from the prior predictive distributions. To start, execute the code in the cell below to sample the prior predictive distributions.       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XF3mh-7F9F0"
      },
      "outputs": [],
      "source": [
        "SEED = 5678\n",
        "with disaster_model:\n",
        "    prior_checks = pymc.sample_prior_predictive(samples=50, random_seed=SEED)\n",
        "prior_checks.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeLgr14uF9F0"
      },
      "source": [
        "There are four distributions we should examine, the switch point, the density of disaster rate, the event rates before and after the switch point. Execute the code in the cell below to display the density plots of each.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1q2msD7F9F0"
      },
      "outputs": [],
      "source": [
        "az.plot_density(prior_checks, group='prior')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQkEAjZkF9F0"
      },
      "source": [
        "Examine the plots noticing the following:     \n",
        "1. The distribution of the switch point is a bimodal, likely a result of the two rates used.    \n",
        "2. The rate density is just based on the counts of the number of incidents per year.     \n",
        "3. The rate densities seem reasonable with long tails.     \n",
        "\n",
        "Overall, these densities look reasonable, given what we know about the problem so far.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nv33878aF9F0"
      },
      "source": [
        "### MCMC sampling the model    \n",
        "\n",
        "Since we initially satisfied that the priors are reasonable, it is time to MCMC sample the model. Execute the code in the cell below to generate the sample traces.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVdzvE8nF9F0"
      },
      "outputs": [],
      "source": [
        "with disaster_model:\n",
        "    # 5000 posterior sample draws\n",
        "    disaster_samples = pymc.sample(draws=5000, return_inferencedata=True, step=[pymc.NUTS(target_accept=0.95,max_treedepth=60)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAvaq6bNF9F0"
      },
      "source": [
        "> **Exercise 29-8:** Notice that the M-H algorithm is used to sample the discrete uniform distribution. Why is it not possible to use the NUTS (or any Hamiltonian-based) sampling algorithm for this distribution?    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkjvE74DF9F0"
      },
      "source": [
        "> **Answer:**              "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-U3zvmgF9F0"
      },
      "source": [
        "Next, execute the code in the cell below to display the trace plots and the parameter distributions.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4caqdF2F9F0"
      },
      "outputs": [],
      "source": [
        "az.plot_trace(disaster_samples);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6R795VHdF9F0"
      },
      "source": [
        "The trace plots look reasonable with constant variation over the sample. The distributions of the rate parameters also appear reasonable. However, the distribution of the switch point has several peaks (multi-modal). This may be a result of the multi-modal prior. None the less, the distribution looks reasonable.   \n",
        "\n",
        "We must also verify that the sampling was effective. To do so we will compute and examine the MCSE and ESS for the sampling. Execute the code in the cell below to display these statistics.       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiVGh9RmF9F1"
      },
      "outputs": [],
      "source": [
        "print(az.summary(disaster_samples, kind='diagnostics'))\n",
        "\n",
        "_,ax = plt.subplots(3,3, figsize=(16,12))\n",
        "az.plot_mcse(disaster_samples, ax=ax[0])\n",
        "az.plot_ess(disaster_samples, kind=\"local\", ax=ax[1]);\n",
        "az.plot_ess(disaster_samples, kind=\"quantile\", ax=ax[2]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07LrEsMeF9F1"
      },
      "source": [
        "> **Exercise 29-9:** Examine the table and plots and answer the following questions:        \n",
        "> 1. Is the MCSE for each of the coefficients reasonable and if not, why not?      \n",
        "> 2. Are the local ESS values reasonable, and can you identify anything odd, and if so, what?     \n",
        "> 3. Are the quantile ESS values reasonable, and can you identify anything odd, and if so, what?      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tttNNiAWF9F1"
      },
      "source": [
        "> **Answers:**     \n",
        "> 1.                   \n",
        "> 2.              \n",
        "> 3.               "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DD0xI-FF9F1"
      },
      "source": [
        "### Posterior distributions   \n",
        "\n",
        "With the traces computed, we can now examine the posterior distribution of the model parameters. Execute the code in the cell below to display the posterior distributions of the model parameters as forest plots and density plots.        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhxMuXH3F9F1"
      },
      "outputs": [],
      "source": [
        "az.plot_forest(disaster_samples, var_names=['switchpoint']);\n",
        "plt.show();\n",
        "az.plot_forest(disaster_samples, var_names=['late_rate', 'early_rate']);\n",
        "plt.show();\n",
        "az.plot_posterior(disaster_samples, var_names=['late_rate', 'early_rate', 'switchpoint']);\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35h4geuzF9F1"
      },
      "source": [
        "> **Exercise 29-10:** Examine the plots above, and answer these questions:       \n",
        "> 1. Given the bit of history for this problem does the HDI of the switch-point seem reasonable and why?     \n",
        "> 2. Is the difference in the rates of disasters before and after the switch point significantly different and why?    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEZTBv4DF9F1"
      },
      "source": [
        "> **Answers:**         \n",
        "> 1.             \n",
        "> 2.                "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhFT0KJHF9F1"
      },
      "source": [
        "### Posterior checks     \n",
        "\n",
        "There is one last aspect of this analysis we should investigate, the posterior predictive analysis. To start, execute the code in the cell below to create the posterior predictive samples.     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtWxvOHXF9F1"
      },
      "outputs": [],
      "source": [
        "SEED = 4466\n",
        "with disaster_model:\n",
        "    pymc.sample_posterior_predictive(disaster_samples, extend_inferencedata=True, random_seed=SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0BZ9gixF9F1"
      },
      "source": [
        "To display the posterior predictive plots execute the code in the cell below.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCzj47PIF9F1"
      },
      "outputs": [],
      "source": [
        "az.plot_ppc(disaster_samples, num_pp_samples=100);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlZHj_amF9F2"
      },
      "source": [
        " think I now understand this problem.  Consider that we want to sample from the joint distribution of our model  $P(x; \\lambda, s)$ to $P(x_{obs})$, where $x$ is the count, $\\lambda = [\\lambda_{early}, \\lambda_{late}]$ are the rate of the point process, and $s= \\{ early,\\ late \\}$ is the binary switch  variable. We can factorize this joint distribution in terms of the likelihood and the joint distribution $P(\\lambda, s)$:\n",
        "\n",
        "\n",
        "$$P(x;\\ \\lambda,\\ s) = P(x\\ | \\lambda,\\ s)\\ P( \\lambda,\\ s)$$\n",
        "\n",
        "Next we can factorize again to get:\n",
        "\n",
        "$$P(x;\\ \\lambda,\\ s ) = P(x\\ |\\ \\lambda,\\ s) P(\\lambda, s ) = P(x\\ |\\ \\lambda,\\ s) P(\\lambda\\ |\\ s)P(s)$$\n",
        "\n",
        "The above makes the dependency on the binary variable s quite explicit.\n",
        "\n",
        "In terms of the Bayesian p-value what does this tell us? Simply that this p-value is conditional on a binary variable, $s$. In other words, I think one needs to find the Bayesian p-value for both states of $s$ independently. Conflating these cases likely gives the biased result we all observe.  \n",
        "\n",
        "It looks like the posterior predictive sampling in PyMC does not give us information on the state of the switch point. So in the end the density comparison I have shows in about the best we can do here.    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H60FtMspF9F2"
      },
      "outputs": [],
      "source": [
        "counts = disaster_samples['posterior_predictive'].disasters.to_numpy().flatten()\n",
        "_,ax=plt.subplots(2,1, figsize=(8,5))\n",
        "ax[0].hist(disaster_data.Count, bins=30, density=True);\n",
        "ax[0].set_title('Density of disasters per year');\n",
        "ax[1].hist(counts, bins=75, density=True);\n",
        "ax[1].set_title('Density of MCMC disasters per year');\n",
        "ax[1].set_xlim(-0.25,6.25)\n",
        "ax[1].set_ylim(0.0,1.6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TTKWXb8F9F2"
      },
      "outputs": [],
      "source": [
        "_,ax = plt.subplots(figsize=(8,3))\n",
        "az.plot_bpv(disaster_samples, kind='u_value', ax=ax);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thUhewGFF9F2"
      },
      "source": [
        "> **Exercise 29-11 :** Examine these plots and answer these questions:     \n",
        "> 1. Do you see reasonable agreement between the ppsterior predictive distribution and the observed values and why? For these data are there discrete blue lines at each disaster count?       \n",
        "> 2. Does the plot of the Bayesian p-value show good agreement with the ideal distribution and what does this tell you about the predictions made by the model?    \n",
        "> 3. Does the plot of the Bayesian u-values show good agreement with the ideal distribution and what does this tell you about the predictions made by the model?    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81er97yuF9F2"
      },
      "source": [
        "> **Answers:**        \n",
        "> 1.          \n",
        "> 2.            \n",
        "> 3.             "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AnDqfqBF9F2"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this lesson you have done the following:\n",
        "\n",
        "- Reviewed the basic properties of a Markov process and Markov chains.\n",
        "- Perform a simple Markov chain Monte Carlo using the Metropolis-Hastings algorithm..\n",
        "- Evaluated the convergence of the model.\n",
        "- Performing prior and posterior checks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rO9n0ruF9F2"
      },
      "source": [
        "#### Copyright 2017, 2018, 2019, 2020, 2022, 2023 Stephen F Elston. All rights reserved."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}